#%%
# -*- coding UTF-8 -*-
'''
@Project : MyPythonProjects
@File : 6.ä½¿ç”¨å±‚çš„åå‘ä¼ æ’­ç®—æ³•ï¼ˆå¯å †å å¤šå±‚ï¼‰.py
@Author : chenbei
@Date : 2021/8/10 14:50
'''
import matplotlib.pyplot as plt
from matplotlib.pylab import mpl

plt.rcParams['font.sans-serif'] = ['Times New Roman']  # è®¾ç½®å­—ä½“é£æ ¼,å¿…é¡»åœ¨å‰ç„¶åè®¾ç½®æ˜¾ç¤ºä¸­æ–‡
mpl.rcParams['font.size'] = 10.5  # å›¾ç‰‡å­—ä½“å¤§å°
mpl.rcParams['font.sans-serif'] = ['SimHei']  # æ˜¾ç¤ºä¸­æ–‡çš„å‘½ä»¤
mpl.rcParams['axes.unicode_minus'] = False  # æ˜¾ç¤ºè´Ÿå·çš„å‘½ä»¤
mpl.rcParams['agg.path.chunksize'] = 10000
plt.rcParams['figure.figsize'] = (7.8, 3.8)  # è®¾ç½®figure_sizeå°ºå¯¸
plt.rcParams['savefig.dpi'] = 600  # å›¾ç‰‡åƒç´ 
plt.rcParams['figure.dpi'] = 600  # åˆ†è¾¨ç‡
from matplotlib.font_manager import FontProperties

font_set = FontProperties(fname=r"C:\Windows\Fonts\simsun.ttc", size=10.5)
import numpy as np
import pandas as pd
import tensorflow as tf
import seaborn as sns
from sklearn.datasets import make_moons
from sklearn.model_selection import train_test_split
X, y = make_moons(n_samples = 2000, noise=0.2, random_state=100) # shape=2000*2
X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3, random_state=42)
#%%
def make_plot(X, y, plot_name,dark=False):
    if (dark):
        plt.style.use('dark_background') # ä½¿ç”¨é»‘è‰²èƒŒæ™¯
    else:
        sns.set_style("whitegrid") # é»˜è®¤ç™½è‰²èƒŒæ™¯
    axes = plt.gca()
    axes.set(xlabel="$x_1$", ylabel="$x_2$")
    plt.title(plot_name, fontsize=30)
    plt.subplots_adjust(left=0.20)
    plt.subplots_adjust(right=0.80)
    # ç»˜åˆ¶æ•£ç‚¹å›¾ï¼Œæ ¹æ®æ ‡ç­¾åŒºåˆ†é¢œè‰² æ•°æ®ç‚¹åˆ†ä¸ºäº†0å’Œ1æ˜ å°„ä¸º2ç§é¢œè‰²
    plt.scatter(X[:, 0], X[:, 1], c=y.ravel(), s=40, cmap=plt.cm.Spectral,edgecolors='none')
    plt.show()
# è°ƒç”¨ make_plot å‡½æ•°ç»˜åˆ¶æ•°æ®çš„åˆ†å¸ƒï¼Œå…¶ä¸­ X ä¸º 2D åæ ‡ï¼Œy ä¸ºæ ‡ç­¾
make_plot(X, y, "Classification Dataset Visualization ",dark=False)
#%% âˆ‘Î³Î©Ï†ÏƒÏ‰Î¸Î»Î´âˆ‚ ÎµÅ‹ É‘ Î³ Î¼ Î²
class MyLayer:
    # å®šä¹‰ç½‘ç»œå±‚
    def __init__(self, n_input, n_neurons, activation=None, weights=None,
                 bias=None):
        '''
        :param n_input: å±‚çš„è¾“å…¥ç‰¹å¾æ•°
        :param n_neurons: å±‚çš„è¾“å‡ºç‰¹å¾æ•°
        :param activation: æ¿€æ´»å‡½æ•° å¤–éƒ¨æŒ‡å®š
        :param weights: æƒå€¼ å†…éƒ¨ç”Ÿæˆ
        :param bias: åç½® å†…éƒ¨ç”Ÿæˆ
        åå‘ä¼ æ’­ç®—æ³•è¯´æ˜ï¼š
        ï¼ˆ1ï¼‰è€ƒè™‘æ— éšå«å±‚çš„å¤šç¥ç»å…ƒè¾“å‡ºèŠ‚ç‚¹OK1,OK2,...,OKk å®é™…æ ‡ç­¾å¯¹åº”ä¸ºt1,t2,..,tk è¾“å‡ºèŠ‚ç‚¹çš„å«ä¹‰ä¸ºç¬¬Kå±‚çš„ç¬¬kä¸ªèŠ‚ç‚¹
        æŸå¤±å‡½æ•° L = 1/2 Î£(OKi-ti)^2   (i=1,2,..k) è¡¨ç¤º ç¬¬Kè¾“å‡ºå±‚çš„kä¸ªèŠ‚ç‚¹è¾“å‡ºä¸é¢„æµ‹çš„ç´¯è®¡æ–¹å·®
        æ¯å±‚ç½‘ç»œå•å…ƒéƒ½ç”±3ä¸ªå°ç¯èŠ‚ç»„æˆ ï¼š ä¸Šä¸€å±‚çš„è¾“å…¥self.last_outputã€æœ¬å±‚æœªæ¿€æ´»çš„è¾“å‡ºz=wx+b ã€æ¿€æ´»è¾“å‡ºo=Ïƒ(z)
        è¾“å‡ºå±‚ä¸ºæœ€åä¸€å±‚æ—¶ï¼Œå¯ä»¥å¿½ç•¥è¾“å‡ºèŠ‚ç‚¹OK1,OK2,...,OKkçš„å¤§æ ‡Kï¼Œç®€åŒ–ä¸ºO1,O2,...,Ok æŸå¤±å‡½æ•° L = 1/2 Î£(Oi-ti)^2 (i=1,2,..k)
        ä¸Šä¸€å±‚çš„è¾“å‡ºself.last_outputä¹Ÿå°±æ˜¯è¾“å…¥å±‚æ•°æ®Xï¼Œè®¾è¾“å…¥å±‚æœ‰jä¸ªèŠ‚ç‚¹ï¼Œè¾“å…¥æ•°æ®ä¸ºx1,x2,..,xjï¼Œè¾“å…¥å±‚ç¬¬jä¸ªèŠ‚ç‚¹åˆ°è¾“å‡ºå±‚ç¬¬kä¸ªèŠ‚ç‚¹çš„æƒå€¼ä¸ºWjk
            â‘  å¼€å§‹è®¡ç®—æŸå¤±å‡½æ•°Lå¯¹æƒå€¼Wjkçš„åå¯¼æ•° ï¼š âˆ‚L/âˆ‚Wjk = 1/2  *  âˆ‚Î£(Oi-ti)^2 / âˆ‚Wjk
            â‘¡ Lå¯¹Wjkçš„åå¯¼æ•°è½¬ä¸ºå…ˆå¯¹è¾“å‡ºOkçš„åå¯¼æ•°å’ŒOkå¯¹Wjkçš„åå¯¼æ•°ä¹˜ç§¯ ï¼š âˆ‚L/âˆ‚Wjk = âˆ‚L/âˆ‚Ok * âˆ‚Ok/âˆ‚Wjk
            â‘¢ è¾“å‡ºOkå¯¹Wjkçš„å¯¼æ•°ç»§ç»­è½¬ä¸ºOkå¯¹Zkçš„åå¯¼æ•°å’ŒZkå¯¹Wjkçš„åå¯¼æ•°ä¹˜ç§¯ ï¼šâˆ‚Ok/âˆ‚Wjk = âˆ‚Ok/âˆ‚Zk * âˆ‚Zk/âˆ‚Wjk
            â‘£ ç»¼åˆâ‘ â‘¡â‘¢å¯ä»¥å¾—åˆ°Lå¯¹è¾“å‡ºå±‚æƒå€¼Wjkçš„åå¯¼æ•°ä¸º ï¼š âˆ‚L/âˆ‚Wjk = âˆ‚L/âˆ‚Ok * âˆ‚Ok/âˆ‚Zk * âˆ‚Zk/âˆ‚Wjk
            â‘¤ âˆ‚L/âˆ‚Ok = æŸå¤±å‡½æ•°çš„åå¯¼æ•° = (Ok-tk) æ³¨ï¼šWjkåªä¸Okæœ‰å…³ï¼Œä¸O1,O2,..Ok-1æ— å…³å¯ä»¥å»æ‰Î£
            â‘¥ âˆ‚Ok/âˆ‚Zk = æ¿€æ´»å‡½æ•°çš„åå¯¼æ•°  ä»¥sigmoid(Zk)ä¸ºä¾‹æœ‰sig'(Zk) = sig(Zk)*(1-sig(Zk))= Ok(1-Ok)
            â‘¦ âˆ‚Zk/âˆ‚Wjk = æœ¬å±‚ç½‘ç»œè¾“å…¥/ä¸Šå±‚ç½‘ç»œè¾“å‡º = âˆ‚(xj*Wjk+bj)/âˆ‚Wjk = xj  ç‰¹åˆ«çš„å¯¹äºæ— éšå«å±‚ä¸Šå±‚ç½‘ç»œè¾“å‡ºä¹Ÿå°±æ˜¯xj
            â‘§ ç»“åˆâ‘¤â‘¥â‘¦å¯ä»¥å¾—åˆ°Lçš„æœ€ç»ˆç»“æœä¸ºï¼šâˆ‚L/âˆ‚Wjk = (Ok-tk)Ok(1-Ok)xj  = Î´k * xj
        ï¼ˆ2ï¼‰å¤šéšå«å±‚çš„æƒ…å†µï¼Œè€ƒè™‘è¾“å…¥å±‚Nï¼Œéšå«å±‚Iã€Jï¼Œä»¥åŠè¾“å‡ºå±‚Kï¼Œå±‚è¾“å‡ºåˆ†åˆ«å¯¹åº”Xnã€Oiã€Ojå’ŒOk
            1ï¼‰ è®¡ç®—Lå¯¹å€’æ•°ç¬¬2å±‚ä¹Ÿå°±æ˜¯Jå±‚çš„æƒå€¼å‚æ•°Wijçš„åå¯¼æ•° ï¼š âˆ‚L/âˆ‚Wij = 1/2  *  âˆ‚Î£(Oi-ti)^2 / âˆ‚Wij
                â‘  é¦–å…ˆè®¡ç®—Lå¯¹Kå±‚è¾“å‡ºOkçš„åå¯¼å’ŒOkå¯¹Wijçš„åå¯¼ ï¼š âˆ‚L/âˆ‚Wij = Î£âˆ‚L/âˆ‚Ok * âˆ‚Oi/âˆ‚Wij (i=1,2,3,..k)
                â‘¡ è®¡ç®—kå±‚è¾“å‡ºOkå¯¹æœªæ¿€æ´»è¾“å‡ºZkçš„åå¯¼å’ŒZkå¯¹Wijçš„åå¯¼ ï¼š âˆ‚Ok/âˆ‚Wij = âˆ‚Ok/âˆ‚Zk * âˆ‚Zk/âˆ‚Wij (i=1,2,3,..k)
                â‘¢ è®¡ç®—kå±‚æœªæ¿€æ´»è¾“å‡ºZkå¯¹Jå±‚è¾“å‡ºOjçš„åå¯¼ ï¼šâˆ‚Zk/âˆ‚Wij = âˆ‚Zk/âˆ‚Oj * âˆ‚Oj/âˆ‚Wij  æ³¨ï¼šZk = Oj * Wjk + bj
                â‘£ ç»“åˆâ‘ â‘¡â‘¢å¯ä»¥å¾—åˆ° âˆ‚L/âˆ‚Wij = âˆ‚L/âˆ‚Ok * âˆ‚Ok/âˆ‚Zk * âˆ‚Zk/âˆ‚Wij = Î£(Ok-tk)Ok(1-Ok) * âˆ‚Zk/âˆ‚Wij
                æ³¨ ï¼š ç¬¬â‘ æ­¥å¯¹âˆ‚L/âˆ‚Wijæ±‚å¯¼æ—¶ä¸èƒ½å»æ‰Î£ç¬¦å·ï¼Œå¯¹Wjkæ±‚å¯¼åªä¸ç¬¬kä¸ªè¾“å‡ºOkæœ‰å…³æ•…å¯ä»¥å»æ‰Î£ï¼Œä½†Wijä¸æ¯ä¸ªOkéƒ½æœ‰å…³ä¸å¯ä»¥å»æ‰Î£
                â‘¤ è®¡ç®— âˆ‚Zk/âˆ‚Wij = âˆ‚(Oj * Wjk + bj)/âˆ‚Wij = Wjk * âˆ‚Oj/âˆ‚Wij  æ³¨ï¼šWijåœ¨è¾“å‡ºOjå½“ä¸­ æ•…Wjkä½œä¸ºå¸¸æ•°
                â‘¥ ç»“åˆâ‘£â‘¤æ­¤æ—¶å¾—åˆ° âˆ‚L/âˆ‚Wij = Î£(Ok-tk)Ok(1-Ok)Wjk * âˆ‚Oj/âˆ‚Wij = Î£Î´k * Wjk * âˆ‚Oj/âˆ‚Wij
                â‘¦ è®¡ç®—Jå±‚è¾“å‡ºOjå¯¹æœªæ¿€æ´»è¾“å‡ºZjçš„åå¯¼å’ŒZjå¯¹Wijçš„åå¯¼ ï¼š âˆ‚Oj/âˆ‚Wij = âˆ‚Oj/âˆ‚Zj * âˆ‚Zj/âˆ‚Wij
                â‘§ è®¡ç®—Jå±‚è¾“å‡ºOjå¯¹æœªæ¿€æ´»è¾“å‡ºZjçš„åå¯¼ ï¼š âˆ‚Oj/âˆ‚Zj = Ïƒ(Zj) * (1-Ïƒ(Zj)) = Oj * (1-Oj)
                â‘¨ è®¡ç®—Jå±‚æœªæ¿€æ´»è¾“å‡ºZjå¯¹Wijçš„åå¯¼ ï¼šâˆ‚Zj/âˆ‚Wij = âˆ‚(Oi * Wij + bi)/âˆ‚Wij  = Oi  æ³¨ï¼šZj = Oi * Wij + bi
                â‘© ç»“åˆâ‘¥â‘¦â‘§å¯å¾— âˆ‚L/âˆ‚Wij = [ Î£Î´k * Wjk * (Oj * (1-Oj) ]* Oi = Î´i * Oi å…¶ä¸­Î´i = (Oj * (1-Oj) * Î£Î´k * Wjk
            2ï¼‰ è®¡ç®—Lå¯¹å€’æ•°ç¬¬3å±‚ä¹Ÿå°±æ˜¯Iå±‚çš„æƒå€¼å‚æ•°Wniçš„åå¯¼æ•° ï¼š âˆ‚L/âˆ‚Wni= 1/2  *  âˆ‚Î£(Oi-ti)^2 / âˆ‚Wni
                â‘  ä»ï¼ˆ1ï¼‰å’Œï¼ˆ2ï¼‰çš„1ï¼‰å¯ä»¥çœ‹å‡ºæŸå¤±å‡½æ•°å¯¹æƒå€¼å‚æ•°çš„åå¯¼å…·æœ‰è§„å¾‹
                   âˆ‚L/âˆ‚Wjk = (Ok-tk)Ok(1-Ok) * Oj  = Î´k * Oj
                   âˆ‚L/âˆ‚Wij = Oj(1-Oj)Î£Î´kWjk * Oi = Î´j * Oi
                   å¯ä»¥å¾—åˆ°ç›¸ä¼¼çš„è§„å¾‹
                   âˆ‚L/âˆ‚Wni = Oi(1-Oi)Î£Î´jWij * On = Î´i * On
                   ...
                   ç”±æ­¤åªè¦ä¾æ¬¡åå‘è®¡ç®—å½“å‰å±‚çš„Î´å‚æ•°å’Œä¸Šå±‚çš„è¾“å‡ºOå³å¯è®¡ç®—æŸå¤±å‡½æ•°å¯¹å½“å‰å±‚çš„æƒå€¼å˜åŒ–ç‡
                â‘¡ ä½¿ç”¨self.last_outputæŒ‡ä»£è¾“å‡ºO,ä¸åŒå±‚å¯¹åº”çš„ä¾æ¬¡ä¸ºOkã€Ojã€Oiã€On...
                â‘¢ å½“ä¸ºè¾“å‡ºå±‚æ—¶ï¼šä½¿ç”¨self.erroræŒ‡ä»£Î´kçš„ä¸€éƒ¨åˆ†ï¼Œå³è¯¯å·®å‡½æ•°çš„å¯¼æ•°(Ok-tk)
                   layer.error = y - output
                   å¦åˆ™å…¶ä»–å±‚æ—¶ï¼šæŒ‡ä»£Î£Î´k*Wjkã€Î£Î´j*Wij ...
                   layer.error = np.dot(next_layer.weights, next_layer.delta)
                â‘£ ä½¿ç”¨self.apply_activation_derivative(layer.last_output)æŒ‡ä»£Ok(1-Ok)ã€Oj(1-Oj)ã€Oi(1-Oi)...(sogmoidå‡½æ•°,å…¶ä»–å‡½æ•°å…·æœ‰å…¶ä»–å½¢å¼)
                â‘¤ ä½¿ç”¨self.deltaæŒ‡ä»£Î´,ä¸åŒå±‚å¯¹åº”çš„ä¾æ¬¡ä¸ºÎ´kã€Î´jã€Î´i...
                   layer.delta = layer.error * layer.apply_activation_derivative(output)  or
                   layer.delta = layer.error * layer.apply_activation_derivative(layer.last_output)

        '''
        # æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–ç½‘ç»œæƒå€¼ shape = din * dout np.random.randn=æ­£æ€åˆ†å¸ƒ
        self.weights = weights if weights is not None else np.random.randn(n_input, n_neurons)
        # shape = (dout,) np.random.rand=å‡åŒ€åˆ†å¸ƒ
        self.bias = bias if bias is not None else np.random.rand(n_neurons)
        self.activation = activation  # æ¿€æ´»å‡½æ•°ç±»å‹ï¼Œå¦‚sigmoid
        self.last_output = None  # æ¿€æ´»å‡½æ•°çš„ä¸Šä¸€å±‚ç½‘ç»œè¾“å‡ºå€¼ o
        self.error = None  # ç”¨äºè®¡ç®—å½“å‰å±‚çš„ delta å˜é‡çš„ä¸­é—´å˜é‡
        self.delta = None  # è®°å½•å½“å‰å±‚çš„ delta å˜é‡ï¼Œç”¨äºè®¡ç®—æ¢¯åº¦
    def activate(self, x):
        # å‰å‘ä¼ æ’­
        z = np.dot(x, self.weights) + self.bias  # (b,n) --> z=x@w+b = (b,n)*(n,i)+ (i,)-->(b,i)*(i,j)+(j,)-->(b,j)*(j,k)+(k,)-->(b,k)
        # é€šè¿‡æ¿€æ´»å‡½æ•°ï¼Œå¾—åˆ°å…¨è¿æ¥å±‚çš„è¾“å‡º o å¹¶å°†è¾“å‡ºèµ‹äºˆself.last_output ç”¨äºåå‘ä¼ æ’­çš„è®¡ç®—
        self.last_output = self._apply_activation(z) # åº”ç”¨æ¿€æ´»å‡½æ•°
        return self.last_output
    def _apply_activation(self, z):
        # è®¡ç®—æ¿€æ´»å‡½æ•°çš„è¾“å‡º
        if self.activation is None:
            return z  # æ— æ¿€æ´»å‡½æ•°ï¼Œç›´æ¥è¿”å›
        # ReLU æ¿€æ´»å‡½æ•°
        elif self.activation == 'relu':
            return np.maximum(z, 0)
        # tanh
        elif self.activation == 'tanh':
            return np.tanh(z)
        # sigmoid
        elif self.activation == 'sigmoid':
            return 1 / (1 + np.exp(-z))
        elif self.activation == 'softmax':
            def softmax():
                if z.ndim == 2:
                    c = np.max(z, axis=1)
                    exp_x = np.exp(z.T - c)
                    return (exp_x / np.sum(exp_x, axis=0)).T
                c = np.max(z)
                exp_z = np.exp(z - c)
                return exp_z / np.sum(exp_z)
            return softmax()
        return z
    def apply_activation_derivative(self, z):
        # è®¡ç®—æ¿€æ´»å‡½æ•°çš„å¯¼æ•°
        # æ— æ¿€æ´»å‡½æ•°ï¼Œå¯¼æ•°ä¸º 1
        if self.activation is None:
            return np.ones_like(z) # è¿”å›ä¸€ä¸ªç”¨0ã€1å¡«å……çš„è·Ÿè¾“å…¥æ•°ç»„å½¢çŠ¶å’Œç±»å‹ä¸€æ ·çš„æ•°ç»„
        # ReLU å‡½æ•°çš„å¯¼æ•°å®ç°
        elif self.activation == 'relu':
            grad = np.array(z, copy=True)
            grad[z > 0] = 1.
            grad[z <= 0] = 0.
            return grad
        # tanh å‡½æ•°çš„å¯¼æ•°å®ç°
        elif self.activation == 'tanh':
            return 1 - z ** 2 # tanh'(x) = 1-tanh^2(x)
        # Sigmoid å‡½æ•°çš„å¯¼æ•°å®ç°
        elif self.activation == 'sigmoid':
            return z * (1 - z) # sigmoid'(x)=sigmoid(x)(1-sigmoid(x))
        return z
class MyNeuralNetwork:
    # å®šä¹‰ç½‘ç»œ
    def __init__(self,learning_rate=0.01,epochs=100):
        self._layers = []  # ç½‘ç»œå±‚å¯¹è±¡åˆ—è¡¨
        self.learning_rate = learning_rate
        self.epochs = epochs
        self.history = {'acu':[],'mse':[]}
    def add_layer(self, layer):
        # è¿½åŠ ç½‘ç»œå±‚
        self._layers.append(layer)
    def fit(self,X_train, y_train):
        self.X = X_train
        self.y = y_train
    def forward(self, X):
        # å‰å‘ä¼ æ’­
        for layer in self._layers:
        # ä¾æ¬¡é€šè¿‡å„ä¸ªç½‘ç»œå±‚
            X = layer.activate(X)
        return X
    def backward(self, X, y):
        output = self.forward(X) # (1,2)
        # åå‘è®¡ç®—æ¯å±‚çš„ğ›¿å˜é‡
        for i in reversed(range(len(self._layers))):  # åå‘å¾ªç¯ len(self._layers)=0,1,2,3-->3,2,1,0=-1,-2,-3,-4
            layer = self._layers[i]  # å¾—åˆ°å½“å‰å±‚å¯¹è±¡
            # å¦‚æœæ˜¯è¾“å‡ºå±‚ i = 3
            if layer == self._layers[-1]:  # å¯¹äºè¾“å‡ºå±‚
                layer.error = y - output  # è®¡ç®— 2 åˆ†ç±»ä»»åŠ¡çš„å‡æ–¹å·®çš„å¯¼æ•°
                # å…³é”®æ­¥éª¤ï¼šè®¡ç®—è¾“å‡ºå±‚çš„ deltaï¼Œå‚è€ƒè¾“å‡ºå±‚çš„æ¢¯åº¦å…¬å¼ âˆ‚L/âˆ‚Wjk = (Ok-tk) * Ok(1-Ok) * Oj  = Î´k * Oj
                # å…¶ä¸­layer.error=(Ok-tk) layer.apply_activation_derivative(output)=Ok(1-Ok)
                layer.delta = layer.error * layer.apply_activation_derivative(output)
            else:
                # i = 2ã€1ã€0
                # i = 2 æ­¤æ—¶next_layerä¸ºè¾“å‡ºå±‚ next_layer.delta=(Ok-tk) * Ok(1-Ok) åœ¨i=3æ—¶å·²ç»ä¼ å…¥å¯ä»¥ä½¿ç”¨
                # è®¡ç®—layer.error=Î£Î´kWjk layer.delta=Oj(1-Oj)Î£Î´kWjk
                # i = 1 æ­¤æ—¶next_layerä¸ºå€’æ•°2å±‚ æ­¤æ—¶ä¹Ÿå·²ä¼ å…¥ åŒç†è®¡ç®—å¾—åˆ°layer.delta=Oi(1-Oi)Î£Î´jWij
                # i = 0 layer.delta=On(1-On)Î£Î´iWni
                next_layer = self._layers[i + 1]  # å¾—åˆ°ä¸‹ä¸€å±‚å¯¹è±¡
                # å…³é”®æ­¥éª¤ï¼šè®¡ç®—éšè—å±‚çš„ deltaï¼Œå‚è€ƒéšè—å±‚çš„æ¢¯åº¦å…¬å¼ âˆ‚L/âˆ‚Wij = Oj(1-Oj) * Î£Î´kWjk * Oi = Î´j * Oi
                # å…¶ä¸­layer.error=Î£Î´kWjk  layer.apply_activation_derivative(layer.last_output)=Oj(1-Oj)

                layer.error = np.dot(next_layer.weights, next_layer.delta)
                layer.delta = layer.error * layer.apply_activation_derivative(layer.last_output)
        # å¾ªç¯æ›´æ–°æƒå€¼
        for i in range(len(self._layers)):
            # layer.weights.shape = n_input * n_neurons + (n_input ,) deltaæ˜¯æ ‡é‡
            # i = 0 è¾“å…¥å½¢çŠ¶(2,) wni=(2,25)  å¯ä»¥å®ç°(2,25)+= (2,25) + (2,)
            # i = 1 è¾“å…¥å½¢çŠ¶(25,) wij=(25,50) å¯ä»¥å®ç°(25,50)+= (25,50) +(25,)
            # i = 2 è¾“å…¥å½¢çŠ¶(50,) wjk=(50,25) å¯ä»¥å®ç°(50,25)+= (50,25) +(50,)
            # i = 3 è¾“å…¥å½¢çŠ¶(25,) wko=(25,2) å¯ä»¥å®ç°(25,2)+= (25,2) +(25,)
            layer = self._layers[i]
            # np.atleast_2d è¡¨ç¤ºå°†è¾“å…¥æ•°ç»„è½¬æ¢ä¸ºè‡³å°‘2ç»´çš„æ•°ç»„
            last_output = np.atleast_2d(X if i == 0 else self._layers[i - 1].last_output)
            # æ¢¯åº¦ä¸‹é™ç®—æ³•ï¼Œdelta æ˜¯å…¬å¼ä¸­çš„è´Ÿæ•°ï¼Œæ•…è¿™é‡Œç”¨åŠ å·
            layer.weights += layer.delta * last_output.T * self.learning_rate
        return output
    def train(self):
        # ç½‘ç»œè®­ç»ƒå‡½æ•°
        # one-hot ç¼–ç 
        self.y_onehot = np.zeros((self.y.shape[0], 2)) # å­˜æ”¾[0,1]æˆ–è€…[1,0] 1600*2
        self.y_onehot[np.arange(self.y.shape[0]), self.y] = 1 # åˆ©ç”¨åæ ‡è¿›è¡Œèµ‹å€¼
        for i in range(self.epochs):  # è®­ç»ƒ 1000 ä¸ª epoch
            self.epoch_output = []
            for j in range(len(self.X)):  # ä¸€æ¬¡è®­ç»ƒä¸€ä¸ªæ ·æœ¬
                output = self.backward(self.X[j], self.y_onehot[j])
                # print("result= %s"%(self.result(output,self.y_onehot[j])))
                self.epoch_output.append(np.argmax(output))
            # æ‰“å°å‡º MSE Loss
            score = self.score()
            mse = self.loss() # æ¯ä¸ªå‘¨æœŸæ•´ä½“éªŒè¯
            print('epoch: %s, mse = %.5f acu = %.5f' % (i+1, float(mse),float(score)))
            self.history['mse'].append(mse)
            self.history['acu'].append(score)
    def loss(self):
        return np.mean(np.square(self.y_onehot - self.forward(self.X)))
    def score(self):
        return np.sum(self.epoch_output == self.y)/len(y)
    def predict_proba(self,X_test):
        return self.forward(X_test)
    def predict_label(self,X_test):
        return np.argmax(self.predict_proba(X_test))
    def result(self,y_pred,y_true):
        y_pred = np.argmax(y_pred)
        y_true = np.argmax(y_true)
        return 'yes' if y_pred == y_true else 'no'
nn = MyNeuralNetwork(learning_rate=0.005,epochs=1000) # å®ä¾‹åŒ–ç½‘ç»œç±»
nn.add_layer(MyLayer(2, 10, 'relu')) # éšè—å±‚ 1, 2 * 25
# nn.add_layer(MyLayer(5, 10, 'relu')) # éšè—å±‚ 2, 25 * 50
# nn.add_layer(MyLayer(50, 25, 'relu')) # éšè—å±‚ 3, 50 * 25
nn.add_layer(MyLayer(10, 2, 'softmax')) # è¾“å‡ºå±‚ 4, 25 * 2
nn.fit(X_train,y_train)
nn.train()





