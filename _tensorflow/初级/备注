设计不同层数、大小的网络模型可以为优化算法提供初始的函数假设空间
但是模型的实际容量可以随着网络参数的优化更新而产生变化
∑γΩφσωθλδ∂ εŋ α
y = 𝛽0 + 𝛽1𝑥 + 𝛽2𝑥2 + 𝛽3𝑥3 + ⋯ + 𝛽𝑛𝑥𝑛 +ε
模型的容量可以通过𝑛简单衡量，在训练的过程中如果网络参数𝛽𝑘+1, . . . , 𝛽𝑛 = 0，那么网络的实际容量退化到𝑘次
通过限制网络参数的稀疏性，可以来约束网络的实际容量
𝑀𝑖𝑛𝑖𝑚𝑖𝑧𝑒 ℒ(𝑓𝜃(x),𝑦), (x,𝑦) ∈ 𝔻𝑡𝑟𝑎𝑖𝑛 --> 𝑀𝑖𝑛𝑖𝑚𝑖𝑧𝑒 ℒ(𝑓𝜃(x), 𝑦) + 𝜆 ∗ 𝛺(𝜃), (x, 𝑦) ∈ 𝔻𝑡𝑟𝑎𝑖𝑛
其中𝛺(𝜃)表示对网络参数𝜃的稀疏性约束函数，一般使用约束𝜃的𝐿范数实现 𝛺(𝜃) = ∑‖𝜃𝑖‖𝑙
较大的𝜆意味着网络的稀疏性更重要；较小的𝜆则意味着网络的训练误差更重要

(1) 早期停止技术 tf.keras.callbacks.EarlyStopping
连续多个周期验证集的监控指标没有提升时就会停止训练

(2) 正则化技术
L0正则化：采用 L0 范数
𝛺(𝜃) = ∑‖𝜃𝑖‖0
L0 范数‖𝜃𝑖‖0 定义为𝜃𝑖中非零元素的个数 L0(vector)=Σnum(xi≠0) i=1,2,..n
loss_reg = tf.reduce_sum(tf.cast(tf.constant(w1),dtype=tf.bool))
+tf.reduce_sum(tf.cast(tf.constant(w2),dtype=tf.bool))
约束∑𝜃𝑖‖𝜃𝑖‖0的大小可以迫使网络中的连接权值大部分为 0，从而降低网络的实际参数量和网络容量
但是L0范数不可导，不可采用梯度下降法优化很少使用

L1正则化：采用 L1 范数
L1 范数‖𝜃𝑖‖1定义为张量𝜃𝑖中所有元素的绝对值之和  vector=[x1,x2,...,xn] L1(vector)=Σ(xi) i=1,2,..,n
L1 正则化也叫 Lasso Regularization，它是连续可导的
eg:
w1 = tf.random.normal([4,3])
w2 = tf.random.normal([4,3])
# 计算每个参数的L1范数，再对所有参数的L1范数求和
loss_reg = tf.reduce_sum(tf.math.abs(w1))+tf.reduce_sum(tf.math.abs(w2))

L2正则化：采用 L2 范数
L2 范数‖𝜃𝑖‖2定义为张量𝜃𝑖中所有元素的平方和 L2(vector)=Σ(xi^2) i=1,2,..,n
loss_reg = tf.reduce_sum(tf.square(w1))+ tf.reduce_sum(tf.square(w2))

(3) Dropout技术
tf.nn.dropout(x, rate)实现某条连接的 Dropout 功能，其中 rate 参数设置断开的概率值 p
其他元素则按照1/(1-rate)进行放大
x = tf.nn.dropout(x, rate=0.5)
也可以将 Dropout 作为一个网络层使用，在网络中间插入一个 Dropout 层
model.add(layers.Dropout(rate=0.5))
直接创建层再添加也可
layer = tf.keras.layers.Dropout(rate, noise_shape=None, seed=None)
model.add(layer)
Dropout 通过随机断开神经网络的连接，减少每次训练时实际参与计算的模型的参数量
在测试时，Dropout 会恢复所有的连接，保证模型测试时获得最好的性能

(4) 数据增强技术(Data Augmentation)
维持样本标签不变，根据先验知识改变样本特征，使新产生的样本也符合符合数据的真实分布
例如对于图片数据对其旋转、缩放、平移、裁剪、改变视角、遮挡某局部区域不会改变图片的类别标签

与当前像素欧式距离≤k/√2的像素点重要性较高，＞k/√2像素点重要性较低
每个输出节点仅与感受野区域内 k*k 个输入节点相连接 而非全连接input_shape*input_shape
此时总的参数量为k*k*dout 例如(28*28)*256 --->(10*10)*256 有2560个参数
这是因为每个输入节点到每个输出节点的权值参数都不同，如果让其相等
权值共享每个输入节点到所有输出节点的参数都相同，那么相当于参数量变成1个输出节点的参数量即k*k

f(x)*g(x)=Σf(τ)g(x-τ)=∫f(τ)g(x-τ)dτ(τ,-∞,+∞)
f(x,y)*g(x,y)=ΣΣf(m,n)g(x-m,y-n)=∫f(m,n)g(x-m,y-n)dmdn (m,n,-∞,+∞)
二维卷积反转 : 做一次左右对称反转和一次上下对称反转
[ a, b, c ]        [ i, h, g ]
[ d, e, f ]  --->  [ f, e, d ]
[ g, h, i ]        [ c, b, a ]

先向右平移再反转等价于先反转再向左平移
例如对于3×3的图像和3×3的卷积核
F(-1,-1) = ΣΣf(m,n)g(-1-m,-1-n) 可以先取定义域-1≤-1-m,-1-n≤1得到-1≤m,n≤0
实际上F(-1,-1)=f(0,0)g(-1,-1)+f(-1,0)g(0,-1)+f(0,-1)g(-1,0)+f(-1,-1)g(0,0)
图像数据和卷积核数据分别为 取向右和向下正方向，坐标位置为
[ 2, 3, 1 ]        [ 0, -1, 0 ]         (-1,-1) (0,-1) (1,-1)
[ 0, 5, 1 ]    *   [ -1, 5, -1 ]        (-1,0) (0,0) (1,0)
[ 1, 0, 8 ]        [ 0, -1, 0 ]         (-1,1) (0,1) (1,1)
则F(-1,-1)=5*0+0*(-1)+3*(-1)+2*5=7 后续依次计算F(0,-1)=7,F(1,-1)=1,F(-1,0)=-8,
F(0,0)=21,F(1,0)=-9,F(-1,1)=5,F(0,1)=-14,F(1,1)=39
可以将g(-1-m,-1-m)理解为对g(m.n)先反转为g(-m,-n)再相反方向平移，即向左和上方平移1个单位
(0)    (-1)    (0)                    (0)    (-1)    (0)                       (0)    (-1)    (0)
(-1)   (5)2     (-1)3    1  ==>       (-1)2   (5)3    (-1)1    ==>        2    3(-1)   1(5)   (-1)
(0)    (-1)0    (0)5     1            (0)0    (-1)5    (0)1               0    5(0)    1(-1)  (0)
           1       0     8               1        0       8               1    0       8

(0)    2(-1)     3(0)    1
(-1)   0(5)     5(-1)    1  ==>
(0)    1(-1)    0(0)     8


标准化数据：sigmoid的导数在x>2或x<-2的范围非常小，容易出现梯度弥散，故有必要将输入数据归一化到[-1,1]
再如 L =w1 * x1 + w2 * x2 + b L对w1和w2的偏导为x1和x2 如果x1和x2相近那么梯度优化方向接近于y=x直达极值点 否则会陡峭
网络层输入𝑥分布相近，并且分布在较小范围内时(如 0 附近)，更有利于函数的优化





