#%%
# -*- coding UTF-8 -*-
'''
@Project : MyPythonProjects
@File : 2.æ‰‹å†™æ•°å­—å›¾ç‰‡åˆ†ç±»é—®é¢˜.py
@Author : chenbei
@Date : 2021/8/3 11:29
'''
import matplotlib.pyplot as plt
from matplotlib.pylab import mpl

plt.rcParams['font.sans-serif'] = ['Times New Roman']  # è®¾ç½®å­—ä½“é£æ ¼,å¿…é¡»åœ¨å‰ç„¶åè®¾ç½®æ˜¾ç¤ºä¸­æ–‡
mpl.rcParams['font.size'] = 10.5  # å›¾ç‰‡å­—ä½“å¤§å°
mpl.rcParams['font.sans-serif'] = ['SimHei']  # æ˜¾ç¤ºä¸­æ–‡çš„å‘½ä»¤
mpl.rcParams['axes.unicode_minus'] = False  # æ˜¾ç¤ºè´Ÿå·çš„å‘½ä»¤
mpl.rcParams['agg.path.chunksize'] = 10000
# plt.rcParams['figure.figsize'] = (7.8, 3.8)  # è®¾ç½®figure_sizeå°ºå¯¸
# plt.rcParams['savefig.dpi'] = 600  # å›¾ç‰‡åƒç´ 
# plt.rcParams['figure.dpi'] = 600  # åˆ†è¾¨ç‡
from matplotlib.font_manager import FontProperties
font_set = FontProperties(fname=r"C:\Windows\Fonts\simsun.ttc", size=10.5)
import numpy as np
import pandas as pd
import tensorflow as tf
from keras.utils.np_utils import to_categorical
import os
# ã€1ã€‘åŠ è½½æ•°æ®
(x_train,y_train) ,(x_test,y_test) = tf.keras.datasets.mnist.load_data() # ndarrayæ ¼å¼
# è½¬æ¢ä¸ºå¼ é‡ å¹¶ç¼©æ”¾åˆ°[-1,1]
x_train = tf.convert_to_tensor(x_train,dtype=tf.float32)/255.
y_train = tf.convert_to_tensor(y_train,dtype=tf.int32)
x_train = tf.reshape(x_train, (-1, 28 * 28))
y_train = tf.one_hot(y_train,depth=10) # æ‰‹å†™æ•°å­—0-9 å…±10ç±»
x_test =  tf.convert_to_tensor(x_test,dtype=tf.float32)/255.
y_test = tf.convert_to_tensor(y_test,dtype=tf.int32)
x_test = tf.reshape(x_test, (-1, 28 * 28))
y_test = tf.one_hot(y_test,depth=10) # æ‰‹å†™æ•°å­—0-9 å…±10ç±»
# [æ•°é‡ç»´åº¦,é«˜åº¦,å®½åº¦,é€šé“æ•°]=[b,h,w,c] ç°åº¦å›¾ç‰‡c=1 RGBc=3
# print(x_train.shape,y_train.shape) # (60000, 28, 28, 1) (60000, 10)
#%% ã€å‰å‘ä¼ æ’­æ‰‹åŠ¨æ­å»ºç½‘ç»œã€‘ oğ‘¢ğ‘¡ = ğ‘Ÿğ‘’ğ‘™ğ‘¢{ğ‘Ÿğ‘’ğ‘™ğ‘¢{ğ‘Ÿğ‘’ğ‘™ğ‘¢[ğ‘‹@ğ‘Š1 + ğ‘1]@ğ‘Š2 + ğ‘2}@ğ‘Š + ğ‘ }
# æœ€ç»ˆç»“è®º ï¼šå­¦ä¹ ç‡è¦å¤§ä¸€äº›=0.1 ä¸”ä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°çš„å¹³å‡å€¼è€Œä¸å®œä½¿ç”¨mseå‡½æ•° å‡†ç¡®ç‡å¾ˆä½ è¿­ä»£æ¬¡æ•°è¦è¶…è¿‡50æ¬¡è¾ƒä¸ºåˆç†
lr = 0.1
Loss = []
Output = []
# è¾“å…¥èŠ‚ç‚¹æ•°ä¸º 784ï¼Œç¬¬ä¸€å±‚çš„è¾“å‡ºèŠ‚ç‚¹æ•°æ˜¯256
# 1) é¦–å…ˆå°† shape ä¸º[ğ‘, 28,28]çš„è¾“å…¥æ•°æ® Reshape ä¸º[ğ‘, 784]
# -1è¡¨ç¤ºè‡ªåŠ¨æ¨å¯¼ é‚£ä¹ˆç­‰äº60000*28*28*1/(28*28) D=2 (60000, 784)
x_train = tf.reshape(x_train, [-1, 28 * 28])
# 2) å®šä¹‰æƒé‡wå’Œåç½®bå‚æ•° ç¬¬ä¸€å±‚åº”å½“æœ‰784*256ä¸ªw bçš„é•¿åº¦ä¸º256 ä¸ªæ•°ä¸º784
# ä½†æ˜¯åªéœ€è¦å®šä¹‰1ä¸ªb ä¸éœ€è¦å®šä¹‰784ä¸ªb
# ä½¿ç”¨tf.Variableè¡¨ç¤ºæ˜¯å¯ä»¥è®­ç»ƒä¼˜åŒ–çš„ bä¸€èˆ¬åˆå§‹åŒ–ä¸º0
w1 = tf.Variable(tf.random.truncated_normal([784, 256], stddev=0.1))  # (784,256)
b1 = tf.Variable(tf.zeros([256]))  # (256,)
# ç¬¬äºŒå±‚çš„è¾“å‡ºèŠ‚ç‚¹æ•°æ˜¯ 128
w2 = tf.Variable(tf.random.truncated_normal([256, 128], stddev=0.1))
b2 = tf.Variable(tf.zeros([128]))
# ç¬¬ä¸‰å±‚çš„è¾“å‡ºèŠ‚ç‚¹æ˜¯ 10
w3 = tf.Variable(tf.random.truncated_normal([128, 10], stddev=0.1))
b3 = tf.Variable(tf.zeros([10]))
epochs = 100
for epoch in range(epochs) :
    with tf.GradientTape() as tape :
        # 3) å¼€å§‹é€å±‚è®¡ç®—çŸ©é˜µ
        # print(b1) # å¯ä»¥è§‚å¯Ÿåˆ°b1ä¸€å¼€å§‹åˆå§‹åŒ–ä¸º0 ä¹‹åb1ç¡®å®è¢«æ›´æ–°
        # (b,784)*(784,256) + (256,)-->(b,256) æ‹“å±•åˆ°bè¡Œ ä¹Ÿå°±æ˜¯bçš„ä¸ªæ•°=è¾“å…¥æ ·æœ¬ä¸ªæ•°
        # h1 shape = b * 256  w1 shape = 784 * 256
        h1 = x_train@w1 + tf.broadcast_to(b1, [x_train.shape[0], 256])
        # æ¿€æ´»å‡½æ•°
        h1 = tf.nn.relu(h1)
        # (b,256)*(256,128) + (128,)-->(b,128)
        # h2 shape = b * 128 w2 shape = 256 *128
        h2 = h1@w2 + b2
        h2 = tf.nn.relu(h2)
        # (b,128)*(128,10) + (128,)-->(b,10)
        # out shape = 60000 * 10  w3 shape = 128 * 10
        out = h2@w3 + b3
        # print(tf.reduce_max(out).numpy(),tf.reduce_min(out).numpy())
        out = tf.nn.softmax(out) # å°†å…¶å½’ä¸€åŒ–åˆ°0-1ä¹‹é—´ ä¸”æ¦‚ç‡å’Œä¸º1
        # æ–­å®šè¾“å‡ºåœ¨0-1ä¹‹é—´ å¦åˆ™æŠ›å‡ºé”™è¯¯
        assert  np.max(out.numpy())<=1 and np.min(out.numpy())>=0
        # loss1 = tf.reduce_mean(tf.square(y_train - out))
        loss1 = tf.keras.metrics.categorical_crossentropy(y_train,out,from_logits=False)# y_predå·²è¢«softmaxè®¾ç½®ä¸ºFalse
        loss1 = tf.reduce_mean(loss1)
        #loss2 = tf.reduce_mean(tf.keras.losses.mse(y_train, out)) # ä¸¤æ¡è®¡ç®—lossçš„ä»£ç å«ä¹‰å®Œå…¨ç›¸
    grads = tape.gradient(loss1, [w1, b1, w2, b2, w3, b3])
    # w1 = w1 - lr * w1_grad æ‰‹åŠ¨æ›´æ–°å‚æ•°
    w1.assign_sub(lr * grads[0])
    b1.assign_sub(lr * grads[1])
    # print(b1) # è¿™æ˜¯ä¸ºäº†å¯¹ç…§ç¬¬ä¸€æ¬¡æ‰“å°b1 æ£€æŸ¥è¿™é‡Œçš„b1æ˜¯å¦è¢«ä¼ é€’åˆ°ä¸‹ä¸€æ¬¡è¿­ä»£çš„b1 ä¸¤ä¸ªåº”è¯¥ç›¸ç­‰
    w2.assign_sub(lr * grads[2])
    b2.assign_sub(lr * grads[3])
    w3.assign_sub(lr * grads[4])
    b3.assign_sub(lr * grads[5])
    Loss.append(loss1.numpy())
    Output.append(out)
    # ç”¨äºéªŒè¯çš„ä»£ç 
    argmax_out = tf.argmax(out, axis=1)
    # argmax_y_train = tf.argmax(y_train, axis=1)
    # tem_bool = tf.argmax(out, axis=1) == tf.argmax(y_train, axis=1) # ç›¸ç­‰ä¸ºTrue
    # tem_int32 = tf.cast(tem_bool, dtype=tf.int32).numpy()
    # tem_int32_count1 = list(tem_int32).count(1)
    # print(tem_int32_count1)
    acu = list((tf.cast(tf.argmax(out, axis=1) == tf.argmax(y_train, axis=1), dtype=tf.int32)).numpy()).count(1) / len(y_train)
    # print(f"ç¬¬{epoch}æ¬¡è¿­ä»£ï¼š mse={round(loss1.numpy(), 5)}  accuracy={round(acu, 5)}")
    print(f"ç¬¬{epoch}æ¬¡è¿­ä»£ï¼š cross_entropy={round(loss1.numpy(), 5)}  accuracy={round(acu, 5)}")
# å°†é¢„æµ‹æ ‡ç­¾å’Œå®é™…æ ‡ç­¾å…ˆè½¬ä¸ºéç‹¬çƒ­ç¼–ç ç”¨äºæ¯”è¾ƒ
actual_label = tf.argmax(y_train,axis=1)
# print(actual_label[0:5],actual_label[-6:-1]) # éªŒè¯æ¯æ¬¡ç¨‹åºè¿è¡Œè®­ç»ƒé›†æ ‡ç­¾æ˜¯å¦æœ‰å˜åŒ– : æ— å˜åŒ–
pre_label = [tf.argmax(Output[i],axis=1) for i in range(len(Output))]
# å¯¹æ¯ä¸ªepochå¾—åˆ°çš„é¢„æµ‹æ ‡ç­¾ä¸å®é™…æ ‡ç­¾è¿›è¡Œæ¯”è¾ƒ åˆ©ç”¨ç±»å‹è½¬æ¢å˜æˆ0-1ç±»å‹
result = [tf.cast(actual_label == pre_label[i] ,dtype=tf.int32) for i in range(len(pre_label))]
accuracy = [list(result[i].numpy()).count(1) / len(actual_label) for i in range(len(result))]
plt.plot(np.arange(0,len(Loss)),Loss,label='cross_entropy')
plt.legend()
plt.show()
plt.plot(np.arange(0,len(Loss)),accuracy,label='accuracy')
plt.legend()
plt.show()
# ç”¨äºéªŒè¯çš„ä»£ç 
# aa = y_train.numpy() # ä¸argmax_y_trainä¿æŒä¸€è‡´
ab = Output[-1].numpy() # é€šè¿‡è§‚å¯Ÿç¡®å®ä¸argmax_outä¸€è‡´
# ac = tf.nn.softmax(Output[-1]).numpy() # è¯æ˜softmaxå¤„ç†å‰åä¸å½±å“æ¦‚ç‡çš„å¤§å°å…³ç³»
# ä»ç»“æœæ¥çœ‹å‡†ç¡®ç‡è™½ç„¶ä¸Šå‡ã€è¯¯å·®ä¹Ÿç¡®å®ä¸‹é™ä½†æ˜¯å‡†ç¡®ç‡å¾ˆä½ æ•…æ›´æ¢losså‡½æ•°ä¸ã€4ã€‘ä¸€è‡´
# ä¹Ÿå¯ä»¥è¿™æ ·è®¡ç®—å‡†ç¡®ç‡ï¼šä½¿ç”¨tf.equalä»£æ›¿== ã€ä½¿ç”¨reduce_sumä»£æ›¿list.count(1)
# acu =[tf.equal(actual_label,pre_label[i]) for i in range(len(pre_label))] # epoch * 60000
# acu = [tf.cast(acu[i],dtype=tf.int32) for i in range(len(acu))] # epoch * 60000
# acu = [tf.reduce_sum(acu[i])/ 60000 for i in range(len(acu))]
#%%
# ã€3ã€‘ä½¿ç”¨å±‚æ­å»ºå‰å‘ç½‘ç»œ å’Œç›´æ¥æ­å»ºç»“æœåŸºæœ¬ç›¸åŒ P54é¡µ
model = tf.keras.Sequential([tf.keras.layers.Dense(256,activation='relu',name='dense_1'),
                            tf.keras.layers.Dense(128,activation='relu',name='dense_2'),
                            tf.keras.layers.Dense(10,name='output',activation='softmax')])
# model.build(input_shape=(len(x_train),28*28))
# model_trainable_variables  = model.trainable_variables
opt = tf.keras.optimizers.SGD(learning_rate=0.1)
# shape=(60000,28,28)--->shape=(60000, 784) æ‰“å¹³äºŒç»´å›¾ç‰‡çŸ©é˜µ
x_train = tf.reshape(x_train, (-1, 28 * 28))
epochs = 10 #
# ç›´æ¥ä½¿ç”¨modelå»fit 10ä»£è¶³ä»¥ é»˜è®¤ä½¿ç”¨batch_size=32
model.compile(optimizer=opt ,loss='CategoricalCrossentropy',metrics= tf.keras.metrics.CategoricalAccuracy())
model.fit(x_train,y_train,epochs=epochs)
model.evaluate(x_test,y_test)
#%%
# æ¯æ¬¡è¿ç®—è¯¯å·®éƒ½ä¸åŒ æ‰€ä»¥å¯ä»¥å°è£…ä¸ºå‡½æ•°ä¼ å…¥è¿­ä»£å‘¨æœŸepochs
def train_epochs(epochs,x_train) :
    Loss = []
    Model_Trainable_Variables = [] # æ¨¡å‹çš„å¯è®­ç»ƒå‚æ•°
    Output = []
    Grads = []
    Trainable_weights = []
    for epoch in range(epochs) :
        with tf.GradientTape() as tape:
            output = model(x_train)  # shape=(60000, 10)
            # æ–­å®šè¾“å‡ºåœ¨0-1ä¹‹é—´ å¦åˆ™æŠ›å‡ºé”™è¯¯
            assert np.max(output.numpy()) <= 1 and np.min(output.numpy()) >= 0
            # loss1 = tf.reduce_mean(tf.keras.losses.mse(y_train, output))
            loss1 = tf.reduce_mean(tf.keras.metrics.categorical_crossentropy(y_train,output,from_logits=False))
            # print(loss1.shape)
        # tf.argmax(output, axis=1) == tf.argmax(y_train, axis=1) æŒ‰è¡Œä¾æ¬¡æ¯”å¯¹å‘é‡ ç›¸ç­‰ä¸ºTrue å¦åˆ™ False
        # tf.cast(* , dtype=tf.int8).numpy() è½¬ä¸ºåªæœ‰0,1çš„å‘é‡
        # ä½¿ç”¨ listè½¬ä¸ºåˆ—è¡¨ ä½¿ç”¨countæ–¹æ³•è®¡ç®—0çš„ä¸ªæ•° ,ç›¸ç­‰æ—¶è½¬ä¸º0 , æ‰€ä»¥0çš„ä¸ªæ•°æ˜¯é¢„æµ‹æ­£ç¡®çš„ä¸ªæ•°
        acu = list((tf.cast(tf.argmax(output, axis=1) == tf.argmax(y_train, axis=1), dtype=tf.int32)).numpy()).count(1) / len(y_train)
        print(f"ç¬¬{epoch}æ¬¡è¿­ä»£ï¼š cross_entropy={round(loss1.numpy(),5)}  accuracy={round(acu,5)}")
        Loss.append(loss1.numpy())
        Model_Trainable_Variables.append(model.trainable_variables)
        Output.append(output)
        Trainable_weights.append(model.trainable_weights)
        #  è®¡ç®—ä¸‰å±‚ç½‘ç»œå‚æ•°çš„æ¢¯åº¦ w1, w2, w3, b1, b2, b3
        grads = tape.gradient(loss1, model.trainable_variables) # gradsçš„ç¡®æœ‰å˜åŒ–
        Grads.append(grads)
        # æ›´æ–°ç½‘ç»œå‚æ•°  w' = w - lr * grad
        grads_and_vars = zip(grads, model.trainable_variables)
        opt.apply_gradients(grads_and_vars=grads_and_vars)
        # print(opt.get_weights())
    # ä»ç»“æœæ¥çœ‹Model_Trainable_Variablesæ¯æ¬¡éƒ½æ˜¯ä¸€æ ·çš„  Gradsä¸ä¸€æ · Trainable_weightsä¸ä¸€æ ·
    return Loss ,Model_Trainable_Variables,Trainable_weights, Grads,Output,model
epochs = 50 # æ‰‹åŠ¨éœ€è¦100ä»£ ä¸”æ²¡æœ‰batch_sizeçš„å‚ä¸ ã€4ã€‘æ›´åƒæ¨¡æ‹Ÿcompileã€fitçš„è¿‡ç¨‹
Loss ,Model_Trainable_Variables,Grads,Trainable_weights,Output,Model= train_epochs(epochs,x_train)
# æ‰€æœ‰å‘¨æœŸè®­ç»ƒæ‰€é¢„æµ‹çš„æ ‡ç­¾ æ¯ä¸ªå‘¨æœŸçš„è¾“å‡ºæ ‡ç­¾æ˜¯60000*10çš„shape æ•… Outputå®é™…ä¸Šæ˜¯epochs*60000*10(ç‹¬çƒ­ç¼–ç )
# Output[i]è¡¨ç¤ºå–å‡ºç¬¬iä¸ªå‘¨æœŸ å¯¹ç¬¬iä¸ªå‘¨æœŸçš„60000*10æŒ‰è¡Œå–æœ€å¤§å€¼çš„ç´¢å¼• å¾—åˆ°epochs*60000çš„shape(æ ‡ç­¾ç¼–ç )
pre_label = [tf.argmax(Output[i],axis=1) for i in range(len(Output)) ]# æ¯è¡Œæœ€å¤§å€¼çš„ç´¢å¼•ä¸ºé¢„æµ‹çš„æ ‡ç­¾ç¼–ç 
actu_label = tf.argmax(y_train.numpy(),axis=1) # å®é™…æ ‡ç­¾çš„ç‹¬çƒ­ç¼–ç ä¹Ÿè½¬æ¢ä¸ºæ ‡ç­¾ç¼–ç 
# ä¸Lossçš„æœ€å1ä¸ªå¯¹çš„ä¸Š
print("æœ€åä¸€æ¬¡çš„è®­ç»ƒäº¤å‰ç†µä¸º: ",tf.reduce_mean(tf.keras.losses.mse(y_train, Output[-1])).numpy())
result = tf.cast(pre_label[-1] == actu_label,dtype=tf.int32)
print("æœ€åä¸€æ¬¡çš„å‡†ç¡®ç‡ä¸º",list(result.numpy()).count(1) / len(actu_label) )
# æ‰€æœ‰å‘¨æœŸå­˜æ”¾çš„å‡†ç¡®ç‡
accuracy = [ tf.cast(pre_label[i] == actu_label,dtype=tf.int32) for i in range(len(Output))]
accuracy = [list(accuracy[i].numpy()).count(1)/len(actu_label) for i in range(len(accuracy))]
plt.plot(np.arange(0,len(Loss)),Loss,label="CrossEntropy")
plt.legend()
plt.show()
plt.plot(np.arange(0,len(accuracy)),accuracy,label="Accuracy")
plt.legend()
plt.show()
# ä½¿ç”¨è¿­ä»£åçš„Modelå†è¿›è¡Œfit
Model.compile(optimizer=opt ,loss='CategoricalCrossentropy',metrics= tf.keras.metrics.CategoricalAccuracy())
Model.fit(x_train,y_train,epochs=5)
Model.evaluate(x_test,y_test)
#%%
# ã€4ã€‘ä¸ä½¿ç”¨complieã€fitè‡ªå®šä¹‰è®­ç»ƒ ä½¿ç”¨batchè®­ç»ƒé›†å®Œæ•´å®šä¹‰æµç¨‹
# 1) å°†æ ‡ç­¾è½¬ä¸ºç‹¬çƒ­ç¼–ç ----(ç‹¬çƒ­ç¼–ç æ²¡æœ‰æ¢¯åº¦ï¼ï¼ï¼ï¼Ÿï¼Ÿï¼Ÿï¼Ÿ)
# -----> å·²è§£å†³: å¹¶éç‹¬çƒ­ç¼–ç æ²¡æœ‰æ¢¯åº¦,åŠ è½½æ•°æ®æ—¶å·²ç»è½¬æ¢ä¸ºç‹¬çƒ­ç¼–ç 
# y_train_onehot = to_categorical(y_train,10) # ---> å¤šä½™æ“ä½œ å˜æˆäº†(60000, 10, 10)æ˜¯é”™è¯¯çš„
# y_test_onehot = to_categorical(y_test,10)
# print(y_train_onehot.shape)
# 2) å°†åŠ è½½çš„x_train/x_test,y_train/y_testå››ä¸ªtensorè½¬æ¢æˆdatasetæ ¼å¼
train_dataset = tf.data.Dataset.from_tensor_slices((x_train,y_train)) # 60000*784å’Œ60000*10(å·²å±•å¹³å’Œç‹¬çƒ­ç¼–ç )
test_dataset = tf.data.Dataset.from_tensor_slices((x_test,y_test)) # 10000*784å’Œ10000*10(å·²å±•å¹³å’Œç‹¬çƒ­ç¼–ç )
# 3) éšæœºæ‰“ä¹±æµ‹è¯•é›†å–ä¸€åŠä½œä¸ºéªŒè¯é›† å–åä¸€åŠéªŒè¯é›†
val_dataset = test_dataset.shuffle(buffer_size=len(x_test)).skip(len(x_test)//2)# 5000*784å’Œ5000*10(å·²å±•å¹³å’Œç‹¬çƒ­ç¼–ç )
test_dataset = test_dataset.take(len(val_dataset)) # å‰©ä¸‹çš„å‰ä¸€åŠä¸ºæµ‹è¯•é›†
# 4) å®šä¹‰batch_size=64 å–è®­ç»ƒé›†å’ŒéªŒè¯é›†å—
batch_size = 64 # ä¸€æ¬¡æŠ“å–çš„æ ·æœ¬æ•° 60000/64=937.5 = 938
# å—æ•°æ®é›†çš„å½¢çŠ¶=(None,784)å’Œ(None,10) Noneæ˜¯å› ä¸ºæœ‰çš„æ‰¹ä¸ä¸€å®šæ˜¯batch_sizeå¤§å°
train_dataset_batch = train_dataset.shuffle(buffer_size=len(train_dataset)).batch(batch_size)
# 5000 // 64 = 79 (None,784)å’Œ(None,10)
val_dataset_batch = val_dataset.shuffle(buffer_size=len(val_dataset)).batch(batch_size)
# 5000 // 64 = 79 (None,784)å’Œ(None,10)
test_dataset_batch = test_dataset.shuffle(buffer_size=len(test_dataset)).batch(batch_size)
# 5) å®šä¹‰ä¼˜åŒ–å™¨ã€æŸå¤±å‡½æ•°å’Œè¯„ä»·å‡½æ•°
optimizer = tf.keras.optimizers.SGD(learning_rate=0.005)
# è¦æ±‚ä¼ å…¥çš„ä¸ºonehotç¼–ç  è¾“å‡ºæ¨¡å‹çš„é¢„æµ‹è¾“å‡ºåœ¨[0,1]å†… æ•…from_logits=Falseä¸éœ€è¦å†è¿›è¡Œç¼©æ”¾
loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)
# å‡†å¤‡metricsè¯„ä»·å‡½æ•°
train_acc_metric = tf.keras.metrics.CategoricalAccuracy()
val_acc_metric = tf.keras.metrics.CategoricalAccuracy()
#%%
# 6) å®šä¹‰æ¨¡å‹
model = tf.keras.Sequential([tf.keras.layers.Dense(256,activation='relu',name='dense_1'),
                            tf.keras.layers.Dense(128,activation='relu',name='dense_2'),
                            tf.keras.layers.Dense(10,name='output',activation='softmax')])
# 7) å‘¨æœŸè¿­ä»£
epochs = 5
epochTrainAcu = [] # shape=(epoch,938)
epochTrainLoss = [] # shape=(epoch,938)
epochValAcu = []# shape=(epoch,79)
for epoch in range(epochs) :
    # è¦æ”¾åœ¨stepå¤–è¾¹ï¼Œepoché‡Œè¾¹ å¦åˆ™ä¼šæŒç»­æ·»åŠ 
    stepValAcu = []  # shape=(79ï¼Œ)
    stepTrainLoss = []  # shape=(938,)
    stepTrainAcu = []  # shape=(938,)
    print(f"å½“å‰è¿­ä»£åˆ°ç¬¬{epoch+1}å‘¨æœŸï¼Œè¯·ç»§ç»­ç­‰å¾…...")

    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset_batch):
        # y_batch_train.shape=(64,10)æˆ–(32,10) x_batch_train = (64,784) æˆ– (32,784)
        # step = 938 train_dataset_batchæœ‰938ä¸ªx_batch_train æ¯æ¬¡éªŒè¯1ä¸ª
        # å¯¹æ¯ä¸ªå—æ•°æ®é›†çš„æ•°æ®è¿›è¡Œè¿­ä»£ å¸¦æœ‰æ•°æ®å’Œæ ‡ç­¾
        # stepä¸ºå—æ•°æ®é›†x_batch_trainçš„å†…éƒ¨æ•°æ®ç¼–å·

        # éªŒè¯x_batch_trainæ˜¯å¦ç›¸åŒ æ ¹æ®60000 = 937 * 64 + 32 æœ€åä¸€ä¸ªbatchå½¢çŠ¶ä¸º(32,784)
        # print(x_batch_train.shape)

        # ç›´è§‚è¯æ˜æ¯ä¸ªå—æ•°æ®é›†ç¡®å®æ˜¯ä¸åŒçš„
        # print(np.mean(np.mean(x_batch_train.numpy(),axis=1)))
        # print(tf.norm(x_batch_train, ord=1).numpy()/ len(x_batch_train)) # ä¹Ÿå¯ä»¥é€šè¿‡è®¡ç®—èŒƒæ•°éªŒè¯

        with tf.GradientTape() as tape:
             # output.shape=(64, 10)
             print(f"æ­£åœ¨è¿è¡Œï¼šç¬¬{epoch+1}å‘¨æœŸçš„ç¬¬{step+1}æ­¥")
             output = model(x_batch_train)
             # print(output.shape,y_batch_train.shape) # éªŒè¯è¾“å‡ºå½¢çŠ¶æ˜¯å¦ç›¸ç­‰

             # é¢„è®¡æ¨¡å‹çš„è¾“å‡ºåº”å½“æ»¡è¶³åœ¨[0,1]ä¹‹é—´ from_logits=False å¦åˆ™æå‰æŠ›å‡ºé”™è¯¯
             assert np.max(output.numpy())<=1 and np.min(output.numpy())>=0

             # æ ¹æ®æŸå¤±å‡½æ•°è®¡ç®—äº¤å‰ç†µæŸå¤±
             step_train_loss_value = loss(y_true=y_batch_train, y_pred=output)
             step_train_acu_value = train_acc_metric(y_true=y_batch_train, y_pred=output)
        # print(f"ç¬¬ %d æ­¥ : loss = %.5f acu = %.5f " % (step,step_train_loss_value.numpy(),step_train_acu_value.numpy()))
        # å½“å‰å‘¨æœŸæ¯æ­¥çš„losså’Œacuå˜åŒ–
        stepTrainLoss.append(step_train_loss_value.numpy())
        stepTrainAcu.append(step_train_acu_value.numpy())
        grads = tape.gradient(step_train_loss_value, model.trainable_variables) # æ¯ä¸€æ­¥éƒ½è®¡ç®—æ¢¯åº¦
        optimizer.apply_gradients(zip(grads, model.trainable_variables)) # æ¯ä¸€æ­¥éƒ½æ›´æ–°å‚æ•°

        # è¯æ˜æ¢¯åº¦å’Œå‚æ•°ç¡®å®æ›´æ–°çš„ä»£ç  å› ä¸ºæœ¬èº«è¿‡å°æ‰€ä»¥ä¸ä½¿ç”¨å¹³å‡è€Œæ˜¯ç”¨æ±‚å’Œ
        def tensor_sum(tensor):
            array = tensor.numpy()
            if array.ndim > 1 :
               return np.round(np.sum(np.sum(array,axis=1)),5)
            else:
                return np.round(np.sum(array),5)
        variables = [tensor_sum(grads[i]) for i in range(6)]
        # print("w1=%.5f b1=%.5f w2=%.5f b2=%.5f w3=%.5f b3=%.5f"
        #       % (variables[0],variables[1],variables[2],variables[3],variables[4],variables[-1]))

    # å¾—åˆ°æ‰€æœ‰å‘¨æœŸçš„å‡†ç¡®ç‡ç»“æœ epoch*step
    epochTrainLoss.append(stepTrainLoss)
    epochTrainAcu.append(stepTrainAcu)
    # é‡ç½®è®­ç»ƒé›†æŒ‡æ ‡ç”¨äºä¸‹ä¸€epochä½¿ç”¨
    train_acc_metric.reset_states()

    # ä½¿ç”¨å½“å‰å‘¨æœŸè¯¥å—æ•°æ®é›†çš„è®­ç»ƒç»“æœå¯¹éªŒè¯é›†è¿›è¡ŒéªŒè¯

    for x_batch_val, y_batch_val in val_dataset_batch:
        # val_dataset_batch.shape= (None,784)å’Œ(None,10)
        # 5000 // 64 = 79  5000 = 64 * 78 + 8
        # x_batch_val.shape=(64,784)æˆ–(8,784)
        # y_batch_val.shape=(64,10)æˆ–(8,10)
        # æ€»val_step = 79
        val_output = model(x_batch_val)
        step_val_acu_value = val_acc_metric(y_batch_val, val_output)
        stepValAcu.append(step_val_acu_value.numpy())
    epochValAcu.append(stepValAcu)
    # é‡ç½®éªŒè¯é›†æŒ‡æ ‡
    val_acc_metric.reset_states()
    if  epoch == epochs - 1 :
        print("è¿­ä»£ç»“æŸ")
#%%
# æµ‹è¯•é›†é¢„æµ‹
# 1) æ•´ä½“è¿›è¡Œé¢„æµ‹ éœ€è¦å¯¹åˆ†å¥½çš„æµ‹è¯•é›†è§£å‹å¾—åˆ°æ•°æ®å’Œæ ‡ç­¾ å› ä¸ºmodel.predictåªæœ‰ä¸€ä¸ªè¾“å…¥ä¸”è¦æ±‚å¼ é‡ç±»å‹
x_test_shuffle,y_test_shuffle = zip(*test_dataset) # è§£å‹å¾—åˆ°2ä¸ªtupleç±»å‹
x_test_shuffle = tf.convert_to_tensor(x_test_shuffle)
y_test_shuffle = tf.convert_to_tensor(y_test_shuffle)
test_pred = model.predict(x_test_shuffle)
equal_bool = tf.equal(tf.argmax(test_pred,axis=1),tf.argmax(y_test_shuffle,axis=1))
equal_int = tf.cast(equal_bool,dtype=tf.int64)
print("æµ‹è¯•é›†çš„å‡†ç¡®ç‡ä¸ºï¼š",tf.reduce_sum(equal_int).numpy()/len(equal_int))
#%%
# 2) æŒ‰batchè¿›è¡Œé¢„æµ‹ ä»¿ç…§éªŒè¯é›†å¾—åˆ°å¤šè¿­ä»£çš„é¢„æµ‹ç‡
Test_ACU = []
for step ,(x_batch_test, y_batch_test) in enumerate(test_dataset_batch):
    test_output = model.predict(x_batch_test)
    test_result = tf.cast(tf.equal(tf.argmax(test_output,axis=1),tf.argmax(y_batch_test,axis=1)), dtype=tf.int64)
    test_acu = tf.reduce_sum(test_result).numpy()/len(test_result)
    print(f"æ­£åœ¨æ‰§è¡Œç¬¬{step + 1}æ¬¡é¢„æµ‹ï¼šacu={test_acu}")
    Test_ACU.append(test_acu)
plt.plot(np.arange(0,len(Test_ACU)),Test_ACU,label="test_acu")
plt.plot([0,len(Test_ACU)],[np.mean(Test_ACU),np.mean(Test_ACU)],label="average_acu")
plt.legend()
plt.show()
#%%
# ç»˜åˆ¶æ¯ä¸ªå‘¨æœŸçš„train_lossã€train_acuçš„æ­¥å˜åŒ–è¶‹åŠ¿
def  plot_epoch_curve(epochAcu,epochs,title,epochLoss=None,lossIsPlot=False) :
    # data =epochTrainAcu
    # plt.rcParams['figure.figsize'] = (10.8, 7.8)
    # plt.figure(figsize=(20, 26))
    fig, ax = plt.subplots()

    for epoch in range(epochs) :
        ax.plot(np.arange(0,len(epochAcu[epoch])),epochAcu[epoch],label=f"epoch={epoch}")
        # if lossIsPlot :
        #     plt.plot(np.arange(0,len(epochLoss[epoch])),epochLoss[epoch],label=f"epoch={epoch}")
    ax.plot(np.arange(0,len(epochAcu[epoch])),np.mean(epochAcu,axis=0),color='k',linewidth=1.5, label="å¹³å‡å‡†ç¡®ç‡")
    ax.set_xlabel('Step')
    ax.set_ylabel('Acu')
    ax.legend(loc="lower right")
    if lossIsPlot :
        ax1 = ax.twinx()
        ax1.set_ylabel('Loss')
        ax1.plot(np.arange(0, len(epochLoss[epoch])), np.mean(epochLoss, axis=0),alpha=0.5,linewidth=0.5, label="å¹³å‡äº¤å‰ç†µæŸå¤±")
        ax1.legend(loc='upper left')
    plt.title(title)
    plt.show()
plot_epoch_curve(epochAcu=epochTrainAcu,epochs=epochs,epochLoss=epochTrainLoss,
                 title='è®­ç»ƒé›†ï¼šå‡†ç¡®ç‡å’Œäº¤å‰ç†µæŸå¤±çš„å‘¨æœŸæ­¥å˜åŒ–',lossIsPlot=True)
# plot_acu_loss_curve(data=epochTrainLoss,epochs=epochs,title="è®­ç»ƒè¯¯å·®çš„å‘¨æœŸæ­¥å˜åŒ–")
plot_epoch_curve(epochAcu=epochValAcu,epochs=epochs,title='éªŒè¯é›†ï¼šå‡†ç¡®ç‡å’Œäº¤å‰ç†µæŸå¤±çš„å‘¨æœŸæ­¥å˜åŒ–')

#%%










