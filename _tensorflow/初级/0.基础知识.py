#%%
# -*- coding UTF-8 -*-
'''
@Project : MyProjects
@File : 0.åŸºç¡€çŸ¥è¯†.py
@Author : chenbei
@Date : 2021/7/22 8:50
'''
import matplotlib.pyplot as plt
from matplotlib.pylab import mpl

plt.rcParams['font.sans-serif'] = ['Times New Roman']  # è®¾ç½®å­—ä½“é£æ ¼,å¿…é¡»åœ¨å‰ç„¶åè®¾ç½®æ˜¾ç¤ºä¸­æ–‡
mpl.rcParams['font.size'] = 10.5  # å›¾ç‰‡å­—ä½“å¤§å°
mpl.rcParams['font.sans-serif'] = ['SimHei']  # æ˜¾ç¤ºä¸­æ–‡çš„å‘½ä»¤
mpl.rcParams['axes.unicode_minus'] = False  # æ˜¾ç¤ºè´Ÿå·çš„å‘½ä»¤
mpl.rcParams['agg.path.chunksize'] = 10000
plt.rcParams['figure.figsize'] = (7.8, 3.8)  # è®¾ç½®figure_sizeå°ºå¯¸
plt.rcParams['savefig.dpi'] = 600  # å›¾ç‰‡åƒç´ 
plt.rcParams['figure.dpi'] = 600  # åˆ†è¾¨ç‡
from matplotlib.font_manager import FontProperties

font_set = FontProperties(fname=r"C:\Windows\Fonts\simsun.ttc", size=10.5)
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow .keras .datasets import fashion_mnist
'''
Epochs : éå†/è®­ç»ƒæ•´ä¸ªè®­ç»ƒé›†çš„æ¬¡æ•°
Iteration : å•æ¬¡éå†/one_epoch è¿›è¡Œçš„è¿­ä»£æ¬¡æ•°
Iterations : æ€»è¿­ä»£æ¬¡æ•°
Batch_Size : å•æ¬¡è¿­ä»£/one_Iteration ä½¿ç”¨çš„æ ·æœ¬æ•°é‡
N_Train : è®­ç»ƒé›†æ ·æœ¬æ•°
å­˜åœ¨å…¬å¼ : N_Train = Iteration * Batch_Size è®­ç»ƒæ ·æœ¬æ•° = å•æ¬¡éå†éœ€è¦çš„è¿­ä»£æ•° * å•æ¬¡è¿­ä»£éœ€è¦çš„æ ·æœ¬æ•°
Iterations = Epochs * Iteration æ€»è¿­ä»£æ¬¡æ•° = æ€»éå†æ¬¡æ•° * å•æ¬¡éå†éœ€è¦çš„è¿­ä»£æ•°
1ã€layerå±‚çš„åˆ†ç±»
2ã€ç¨€ç–åˆ†ç±»äº¤å‰ç†µ
3ã€å›è°ƒä¿å­˜æ¨¡å‹ 
4ã€æ—©æœŸåœæ­¢æŠ€æœ¯
5ã€evaluateå’Œpredictçš„åŒºåˆ«
6ã€strå’Œreprçš„åŒºåˆ«
7ã€mapå‡½æ•°çš„ä½¿ç”¨
8ã€å»ºç«‹å›¾åƒé€šé“
9ã€shuffleã€repeatã€batchçš„å…ˆåé¡ºåºé—®é¢˜
10ã€next(iter(iterable),default)
11ã€CNNå·ç§¯è¿‡ç¨‹
12ã€å¸¸è§OPè¿ç®—
13ã€tf.data.Datasetå¯¹è±¡
14ã€æ ‡å‡†çš„ETLè¿‡ç¨‹
'''
# ã€1ã€‘layerå±‚çš„åˆ†ç±»
# å…³äºå±‚å¤§ç±»: ç½‘ç»œå±‚Coreã€å·ç§¯å±‚Convolutiionalã€æ± åŒ–å±‚Poolingã€å±€éƒ¨è¿æ¥å±‚/é€’å½’å±‚Recurrentã€åµŒå…¥å±‚Embeddingã€é«˜çº§æ¿€æ´»å±‚ã€å™ªå£°å±‚ã€åŒ…è£…å±‚
# <1>ç½‘ç»œå±‚Core:
# 1) Denseå…¨è¿æ¥å±‚ : keras.layers.core.Dense()  æœ¬å±‚å®ç°çš„è¿ç®—ä¸ºoutput=activation(dot(input,kernel)+bias)
# 2) Activationæ¿€æ´»å±‚ :ã€€keras.layers.core.Activation(activation) æ¿€æ´»å±‚å¯¹ä¸€ä¸ªå±‚çš„è¾“å‡ºæ–½åŠ æ¿€æ´»å‡½æ•°
# 3) Dropoutå±‚ : keras.layers.core.Dropout(rate, noise_shape=None, seed=None) Dropoutå±‚ç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆ
# ä¸ºè¾“å…¥æ•°æ®æ–½åŠ Dropout,Dropoutå°†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ¯æ¬¡æ›´æ–°å‚æ•°æ—¶æŒ‰ä¸€å®šæ¦‚ç‡ï¼ˆrateï¼‰éšæœºæ–­å¼€è¾“å…¥ç¥ç»å…ƒ
# 4) Flattenå±‚ : keras.layers.core.Flatten() Flattenå±‚ç”¨æ¥å°†è¾“å…¥â€œå‹å¹³â€ï¼Œå³æŠŠå¤šç»´çš„è¾“å…¥ä¸€ç»´åŒ–ï¼Œå¸¸ç”¨åœ¨ä»å·ç§¯å±‚åˆ°å…¨è¿æ¥å±‚çš„è¿‡æ¸¡
# 5) Reshapeå±‚
# 6) Permuteå±‚
# 7) RepeatVectorå±‚
# 8) Lambdaå±‚
# 9) ActivityRegularizerå±‚
# 10) Maskingå±‚
# <2>å·ç§¯å±‚Convolutional
# 1)ä¸€ç»´å·ç§¯å±‚Conv1D : keras.layers.convolutional.Conv1D
# 2)äºŒç»´å·ç§¯å±‚Conv2D : keras.layers.convolutional.Conv2D
# 3)SeparableConv2Då±‚ : keras.layers.convolutional.SeparableConv2D æ·±åº¦æ–¹å‘ä¸Šçš„å¯åˆ†ç¦»å·ç§¯
# 4)Conv2DTransposeå±‚ : keras.layers.convolutional.Conv2DTranspos è½¬ç½®çš„å·ç§¯æ“ä½œï¼ˆåå·ç§¯ï¼‰
# 5)Conv3Då±‚ : keras.layers.convolutional.Conv3D ä¸‰ç»´å·ç§¯å¯¹ä¸‰ç»´çš„è¾“å…¥è¿›è¡Œæ»‘åŠ¨çª—å·ç§¯
# 6)Cropping1Då±‚ : keras.layers.convolutional.Cropping1D åœ¨æ—¶é—´è½´ï¼ˆaxis1ï¼‰ä¸Šå¯¹1Dè¾“å…¥ï¼ˆå³æ—¶é—´åºåˆ—ï¼‰è¿›è¡Œè£å‰ª
# 7)Cropping2Då±‚ : keras.layers.convolutional.Cropping2D å¯¹2Dè¾“å…¥ï¼ˆå›¾åƒï¼‰è¿›è¡Œè£å‰ªï¼Œå°†åœ¨ç©ºåŸŸç»´åº¦ï¼Œå³å®½å’Œé«˜çš„æ–¹å‘ä¸Šè£å‰ª

# ã€2ã€‘ç¨€ç–åˆ†ç±»äº¤å‰ç†µ
# æŸå¤±å‡½æ•°ä½¿ç”¨ç¨€ç–åˆ†ç±»äº¤å‰ç†µSparseCategoricalCrossentropyæˆ–sparse_categorical_crossentropy
loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True)
# éƒ½éœ€è¦ä¼ å…¥éonehotç¼–ç çš„y_true, y_pred
# tf.keras.losses.sparse_categorical_crossentropyæ˜¯ç›´æ¥ä¼ å…¥,
# tf.keras.losses.SparseCategoricalCrossentropyæ˜¯ç±»å®ä¾‹åŒ–åå†ä¼ å…¥
# @@ categorical_crossentropy è¦æ±‚targetä¸ºonehotç¼–ç 
# from_logits = False è¡¨ç¤ºè¾“å…¥è¿›æ¥çš„y_predå·²ç¬¦åˆæŸç§åˆ†å¸ƒ, ç³»ç»Ÿåªä¼šå¸®ä½ æŠŠæ¦‚ç‡å½’ä¸€åŒ–ã€‚æ¯”å¦‚æŠŠ[ 0.2 , 0.6 ]å˜æˆ[0.25, 0.75]
# from_logits = True è¡¨ç¤ºæ˜¯åŸå§‹æ•°æ®ï¼Œç³»ç»Ÿä¼šå¸®ä½ åšsoftmaxåå†è¿›è¡Œè®¡ç®—

# ã€3ã€‘å›è°ƒä¿å­˜æ¨¡å‹
# tf.keras.callbacks.ModelCheckpoint å…è®¸åœ¨è®­ç»ƒçš„è¿‡ç¨‹ä¸­å’Œç»“æŸæ—¶å›è°ƒä¿å­˜çš„æ¨¡å‹
checkpoint_path = "training_1/cp.ckpt"
tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_path, monitor='val_loss', verbose=0, save_best_only=False,
    save_weights_only=False, mode='auto', save_freq='epoch',
    options=None)

# ã€4ã€‘æ—©æœŸåœæ­¢
stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)
# patience : è¶…è¿‡patienceä¸ªå‘¨æœŸæ²¡æœ‰æ»¡è¶³æœ€å°æ”¹å–„æ¡ä»¶å³åœæ­¢
# monitor : ç›‘æ§çš„æŒ‡æ ‡
# min_delta :ã€€æœ€å°æ”¹å–„æ¡ä»¶ å½“è®­ç»ƒé›†ä¸Šçš„lossä¸åœ¨å‡å°(å³å‡å°çš„ç¨‹åº¦å°äºæŸä¸ªé˜ˆå€¼)åˆ™åœæ­¢è®­ç»ƒ

# ã€5ã€‘ evaluateå’Œpredictçš„åŒºåˆ«
# 1.è¾“å…¥è¾“å‡ºä¸åŒ
# model.evaluate  æµ‹è¯•é›†æ•°æ®å’Œæµ‹è¯•é›†æ ‡ç­¾,ç„¶åå°†é¢„æµ‹ç»“æœä¸çœŸå®æ ‡ç­¾ç›¸æ¯”è¾ƒ,å¾—åˆ°ä¸¤è€…è¯¯å·®å¹¶è¾“å‡º
# model.predict   è¾“å…¥æµ‹è¯•é›†æ•°æ®(data)ï¼Œè¾“å‡ºé¢„æµ‹ç»“æœ
# 2.æ˜¯å¦éœ€è¦çœŸå®æ ‡ç­¾
# model.evaluate  éœ€è¦ï¼Œå› ä¸ºéœ€è¦æ¯”è¾ƒé¢„æµ‹ç»“æœä¸çœŸå®æ ‡ç­¾çš„è¯¯å·®
# model.predict   ä¸éœ€è¦ï¼Œåªæ˜¯å•çº¯è¾“å‡ºé¢„æµ‹ç»“æœï¼Œå…¨ç¨‹ä¸éœ€è¦æ ‡ç­¾çš„å‚ä¸

# ã€6ã€‘strå’Œreprçš„åŒºåˆ«
# str()å’Œrepr()éƒ½å¯ä»¥å°†pythonä¸­çš„å¯¹è±¡è½¬æ¢ä¸ºå­—ç¬¦  ä¸¤è€…ä¹‹é—´çš„ç›®æ ‡ä¸åŒ :
# str()ä¸»è¦é¢å‘ç”¨æˆ·ï¼Œå…¶ç›®çš„æ˜¯å¯è¯»æ€§ï¼Œè¿”å›å½¢å¼ä¸ºç”¨æˆ·å‹å¥½æ€§å’Œå¯è¯»æ€§éƒ½è¾ƒå¼ºçš„å­—ç¬¦ä¸²ç±»å‹ï¼›
# repr()é¢å‘çš„æ˜¯pythonè§£é‡Šå™¨ï¼Œæˆ–è€…è¯´å¼€å‘äººå‘˜ï¼Œå…¶ç›®çš„æ˜¯å‡†ç¡®æ€§ï¼Œå…¶è¿”å›å€¼è¡¨ç¤ºpythonè§£é‡Šå™¨å†…éƒ¨çš„å«ä¹‰ï¼Œå¸¸ä½œä¸ºç¼–ç¨‹äººå‘˜debugç”¨é€”ã€‚
# repr()çš„è¿”å›å€¼ä¸€èˆ¬å¯ä»¥ç”¨eval()å‡½æ•°æ¥è¿˜åŸå¯¹è±¡ï¼Œé€šå¸¸æ¥è¯´æœ‰å¦‚ä¸‹ç­‰å¼ï¼š
# obj = eval(repr(obj))
#%%
# ã€7ã€‘mapå‡½æ•°çš„ä½¿ç”¨
# map(function, iterable, ...)function -- å‡½æ•° iterable -- ä¸€ä¸ªæˆ–å¤šä¸ªåºåˆ—
# å½“éœ€è¦å¯¹åºåˆ—è¿›è¡ŒæŸäº›æ“ä½œæˆ–è€…å¤„ç†ï¼Œå°†å…¶è½¬æ¢ä¸ºæ–°çš„åˆ—è¡¨æ—¶ï¼Œç”¨mapå‡½æ•°
# ä»¥å‚æ•°åºåˆ—ä¸­çš„æ¯ä¸€ä¸ªå…ƒç´ è°ƒç”¨ function å‡½æ•°ï¼Œè¿”å›åŒ…å«æ¯æ¬¡ function å‡½æ•°è¿”å›å€¼çš„æ–°åˆ—è¡¨æˆ–è€…è¿­ä»£å™¨
def square(x) :
     return x ** 2
print(map(square, [1,2,3,4,5])) # <map object at 0x000001ED52D97780>
# éœ€è¦ä½¿ç”¨listå°†å…¶è½¬æ¢è¾“å‡º
print(list(map(square, [1,2,3,4,5])))

# ã€8ã€‘å»ºç«‹å›¾åƒé€šé“
# å‡è®¾æˆ‘ä»¬å·²ç»æœ‰äº†ä¸€ä¸ªåŒ…å«æ‰€æœ‰JPEGå›¾åƒåç§°çš„åˆ—è¡¨å’Œä¸€ä¸ªä¸ä¹‹å¯¹åº”çš„æ ‡ç­¾åˆ—è¡¨ã€‚
# é€šé“å»ºç«‹æ­¥éª¤å¦‚ä¸‹ï¼š
# 1) ä»æ–‡ä»¶åå’Œæ ‡ç­¾çš„åˆ‡ç‰‡åˆ›å»ºæ•°æ®é›†
# 2) ä½¿ç”¨é•¿åº¦ç­‰äºæ•°æ®é›†å¤§å°çš„buffer sizeï¼Œæ‰“ä¹±æ•°æ®é›†ã€‚è¿™ç¡®ä¿äº†è‰¯å¥½çš„æ”¹ç»„ã€‚
# 3) ä»å›¾åƒæ–‡ä»¶åä¸­è§£æåƒç´ å€¼ã€‚ä½¿ç”¨å¤šçº¿ç¨‹æå‡é¢„å¤„ç†çš„é€Ÿåº¦
# 4)ï¼ˆå¯é€‰æ“ä½œï¼‰å›¾åƒæ•°æ®æ‰©å¢ã€‚ä½¿ç”¨å¤šçº¿ç¨‹æå‡é¢„å¤„ç†çš„é€Ÿåº¦ã€‚
# 5) æ‰¹é‡å¤„ç†å›¾ç‰‡
# 6) é¢„å–ä¸€ä¸ªæ‰¹æ¬¡ä»¥ç¡®ä¿æ‰¹å¤„ç†å¯ä»¥éšæ—¶ä½¿ç”¨
# dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))
# dataset = dataset.shuffle(len(filenames))
# dataset = dataset.map(parse_function, num_parallel_calls=4)
# dataset = dataset.map(train_preprocess, num_parallel_calls=4)
# dataset = dataset.batch(batch_size)
# dataset = dataset.prefetch(1)
def parse_function(filename, label):
    # è¯»å–å›¾åƒæ–‡ä»¶å†…å®¹
    # ä½¿ç”¨JPEGå›¾åƒæ ¼å¼è§£ç 
    # è½¬åŒ–ä¸º0åˆ°1çš„æµ®ç‚¹å€¼
    # ä¿®æ”¹å°ºå¯¸åˆ°ï¼ˆ64ï¼Œ 64ï¼‰
    image_string = tf.io.read_file(filename)
    image = tf.image.decode_jpeg(image_string, channels=3)
    image = tf.image.convert_image_dtype(image, tf.float32)
    resized_image = tf.image.resize(image, [64, 64])
    return resized_image, label
def train_preprocess(image, label):
    # å‡½æ•°train_preprocessï¼ˆoptionallyï¼‰å¯ç”¨äºæ‰§è¡Œæ•°æ®æ‰©å¢
    # ä»¥1 / 2çš„æ¦‚ç‡æ°´å¹³ç¿»è½¬å›¾åƒ,åº”ç”¨éšæœºäº®åº¦å’Œé¥±å’Œåº¦
    image = tf.image.random_flip_left_right(image)
    image = tf.image.random_brightness(image, max_delta=32)
    image = tf.image.random_saturation(image, lowe=0.5, upper=1.5)
    image = tf.clip_by_value(image, 0.0, 1.0)
    return image, label

# ã€9ã€‘ shuffleã€repeatã€batchçš„å…ˆåé¡ºåºé—®é¢˜
# å‚è€ƒç½‘å€ :ã€€https://blog.csdn.net/lichaobxd/article/details/106476115
# æ•°æ®å¤„ç†çš„éœ€æ±‚ : è¢«å……åˆ†æ‰“ä¹±; è¢«åˆ†å‰²ä¸º batch;æ°¸è¿œé‡å¤;å°½å¿«æä¾› batch ä½¿ç”¨tf.dataæ–¹æ³•å³å¯
# åœ¨ .repeat ä¹‹å .shuffleï¼Œä¼šåœ¨ epoch ä¹‹é—´æ‰“ä¹±æ•°æ® æœ‰äº›æ•°æ®å‡ºç°ä¸¤æ¬¡çš„æ—¶å€™ï¼Œå…¶ä»–æ•°æ®è¿˜æ²¡æœ‰å‡ºç°è¿‡
# åœ¨ .batch ä¹‹å .shuffleï¼Œä¼šæ‰“ä¹± batch çš„é¡ºåºï¼Œä½†æ˜¯ä¸ä¼šåœ¨ batch ä¹‹é—´æ‰“ä¹±æ•°æ®
# ä¾‹å¦‚np.arange(20)æœ‰20ä¸ªå…ƒç´ 
dataset = tf.data.Dataset.range(10)
# â‘  shuffle->batch(3)->repeat(2)
# å¦‚ç”Ÿæˆ[ [0,1,2,3,4],[5,6,7,8,9],[15,16,17,18,19] ] å’Œ [ [10,11,12,13,14] ]
# æ‰€æœ‰æ•°æ®å…ˆæ‰“ä¹±ï¼Œç„¶åæ‰“åŒ…æˆbatchè¾“å‡ºï¼Œæ•´ä½“æ•°æ®é‡å¤2ä¸ªepoch
# å…ˆæ‰“åŒ…3ä¸ª,å†é‡å¤æ‰“åŒ…åªèƒ½æ‰¾å‰©ä¸‹çš„,1.ä¸€ä¸ªbatchä¸­çš„æ•°æ®ä¸ä¼šé‡å¤ï¼›2.æ¯ä¸ªepochçš„æœ€åä¸€ä¸ªbatchçš„å°ºå¯¸å°äºç­‰äºbatch_size
list((dataset.shuffle(10).batch(3).repeat(2)).as_numpy_iterator())
# â‘¡ shuffle->repeat(3)->batch
# å¦‚ç”Ÿæˆ [ [0,1,2,3,4],[10,11,12,13,14],[5,6,7,8,9] ] å’Œ [ [15,16,17,18,19],[15,16,17,18,19],[10,11,12,13,14] ]
# é‡å¤æ‰“ä¹±2æ¬¡æ•°æ®é›†è¿›è¡Œæ‹¼æ¥, å†è¿™äº›æ•°æ®éšæœºè¿›è¡Œæ‰“åŒ…
# batchçš„æ•°æ®å¯èƒ½é‡å¤ï¼Œæ¯ä¸ªepochçš„æœ€åä¸€ä¸ªbatchçš„å°ºå¯¸å°äºç­‰äºbatch_size
eam = dataset.shuffle(10).repeat(2)
print(list(eam.as_numpy_iterator()))
print(list(eam.batch(3).as_numpy_iterator()))
# â‘¢ batch(3)->repeat(2)->shuffle
# å¦‚ç”Ÿæˆ[ [0,1,2,3,4],[5,6,7,8,9],[10,11,12,13,14] ] ; [ [0,1,2,3,4],[5,6,7,8,9],[10,11,12,13,14] ]
# ä»¥åŠ [ [15,16,17,18,19] ] ; [ [15,16,17,18,19] ]
# å…ˆæŠŠæ‰€æœ‰æ•°æ®æŒ‰æŒ‡å®šå¤§å°ä¾æ¬¡å…ˆæ‰“åŒ…æˆbatchï¼Œç„¶åæŠŠæ‰“åŒ…æˆbatchçš„æ•°æ®é‡å¤ä¸¤éï¼Œæœ€åå†å°†æ‰€æœ‰batchæ‰“ä¹±è¿›è¡Œè¾“å‡º
# 1.æ‰“ä¹±çš„æ˜¯batchï¼›2.æŸäº›batchçš„å°ºå¯¸å°äºç­‰äºbatch_size
eam = dataset.batch(3).repeat(2)
print(list(eam.as_numpy_iterator()))
print()
print(list(eam.shuffle(len(eam)).as_numpy_iterator()))
# ã€10ã€‘ next(iter(iterable),default)
# å¯¹å¯è¿­ä»£å¯¹è±¡ç”Ÿæˆè¿­ä»£å™¨å†ä½¿ç”¨nextå¯ä»¥å¾—åˆ°è¿­ä»£è¾“å‡º

#%%
# ã€11ã€‘ CNNå·ç§¯è¿‡ç¨‹
# è¾“å…¥å›¾åƒå°ºå¯¸ N1 * N1 * D1  è¾“å‡ºå°ºå¯¸ Y * Y * D2
# è¾“å…¥é€šé“çš„é€šé“æ•°D1å†³å®šäº†å·ç§¯æ ¸çš„é€šé“æ•°D1 æ— è®ºè¾“å…¥é€šé“æ•°D1å¤šå°‘ 1ä¸ªå·ç§¯æ ¸åªèƒ½å¾—åˆ°1ä¸ªè¾“å‡ºçŸ©é˜µ å› ä¸ºå¤šé€šé“çŸ©é˜µè¿˜è¦ç»§ç»­å¯¹åº”ç›¸åŠ 
# ä¸€ä¸ªå›ºå®šçš„å·ç§¯æ ¸åªèƒ½å®ŒæˆæŸç§é€»è¾‘çš„ç‰¹å¾æå–ï¼Œä¾‹å¦‚å›¾ç‰‡çš„äº®åº¦ï¼Œ å¦‚æœéœ€è¦æå–å¤šä¸ªç»´åº¦çš„ç‰¹å¾è¯¸å¦‚å›¾ç‰‡çš„é¢œè‰²ã€åˆ†å¸ƒã€é¥±å’Œåº¦ç­‰ç­‰å¯è®¾ç½®å¤šä¸ªå·ç§¯æ ¸
# å·ç§¯æ ¸å°ºå¯¸ N2 * N2 * D1  å·ç§¯æ ¸ä¸ªæ•° K   æ»‘åŠ¨æ­¥é•¿ S   é›¶å€¼å¡«å……æ‹“å±•ç»´åº¦ zero-padding = P (é’ˆå¯¹å›¾ç‰‡æ‹“å±•)
# è®¾ç½®äº†æ­¥é•¿ä¹‹åï¼Œå¾ˆæœ‰å¯èƒ½æŸäº›ä½ç½®æ»‘ä¸åˆ°ï¼Œä¸ºäº†é¿å…äº†è¾¹ç¼˜ä¿¡æ¯è¢«ä¸€æ­¥æ­¥èˆå¼ƒçš„é—®é¢˜ï¼Œæˆ‘ä»¬éœ€è¦è®¾ç½®å¡«å……å€¼æ¥è§£å†³è¿™ä¸ªé—®é¢˜
# Y = (N1 - N2 + 2P ) / S + 1   D2 = K è¾“å‡ºå±‚çš„ä¸ªæ•° D2 å–å†³äºä¸Šä¸€å±‚å·ç§¯æ ¸çš„ä¸ªæ•° K è€Œéé€šé“æ•° D1ï¼ŒKä¸ªå·ç§¯æ ¸æœ‰Kä¸ªè¾“å‡ºçŸ©é˜µYä»£è¡¨ä¸åŒçš„ç»´åº¦ç‰¹å¾
# å¤šè¾“å…¥å¤šå·ç§¯æ ¸çš„è¾“å‡ºYå…·æœ‰ D2 ä¸ªç‰¹å¾å›¾ï¼Œç±»ä¼¼çš„ä¹Ÿå¯ä»¥ä½¿ç”¨é€šé“çš„æ¦‚å¿µï¼Œå³è¾“å‡ºå½¢çŠ¶ä¸º Y * Y * D2 (stackæ‹¼æ¥æ“ä½œ)
# å·ç§¯æ ¸1-->å¼ é‡Yé€šé“1,å·ç§¯æ ¸2-->å¼ é‡Yé€šé“2,...,å·ç§¯æ ¸K-->å¼ é‡Yé€šé“K (è§ä¹¦P226é¡µçš„ç¤ºæ„å›¾)
# æ¯ä¸ªå·ç§¯æ ¸çš„å¤§å° N2ï¼Œæ­¥é•¿ Sï¼Œå¡«å……è®¾å®š P ç­‰éƒ½æ˜¯ç»Ÿä¸€è®¾ç½®ï¼Œè¿™æ ·æ‰èƒ½ä¿è¯è¾“å‡ºçš„æ¯ä¸ªé€šé“å¤§å°ä¸€è‡´ï¼Œä»è€Œæ»¡è¶³æ‹¼æ¥çš„æ¡ä»¶
# å¸¸è§„è®¾ç½® D1 = 3 S = 1 P = 1
# å¤šé€šé“æ—¶å•ä¸ªå·ç§¯æ ¸çš„ä¸åŒé€šé“å·ç§¯çŸ©é˜µæ˜¯ä¸ç›¸åŒçš„ï¼Œä½†æ˜¯å¤šä¸ªå·ç§¯æ ¸ä¹‹é—´å¯ä»¥æ˜¯ç›¸åŒçš„ï¼Œä¹Ÿå°±æ˜¯å·ç§¯æ ¸æƒå€¼å…±äº«ï¼Œå•å’Œå¤šé€šé“éƒ½å¯ä»¥æœ‰å¤šä¸ªå·ç§¯æ ¸
# å¯¹äºç°åº¦å›¾å•é€šé“, å·ç§¯æ ¸ç”±äºå…±äº«æœºåˆ¶ å·ç§¯æ ¸çŸ©é˜µæˆ–è€…è¯´æƒé‡çŸ©é˜µä¸ªæ•°å°±æ˜¯ N2 * N2 å¯¹äºå¤šé€šé“ N2 * N2 * D1
# æ¯ä¸ªå·ç§¯æ ¸æœ‰ 1 ä¸ªåç½® é‚£ä¹ˆæ€»çš„å‚æ•°ä¸ªæ•°ä¸º N2 * N2 * D1 * K ä¸ªæƒé‡å‚æ•°å’Œ K ä¸ªåç½®å‚æ•°
# è‹¥æƒ³è¦ä¿æŒè¾“å…¥è¾“å‡ºå°ºå¯¸ä¸å˜ å³ Y = N1 ; å›ºå®š S = 1 æ—¶å– P = 1 N2 = 3ã€ P = 2 æ—¶ å– N2 = 5 å³ N2 = 2 * P + 1
# åœ¨ S = 1 æ—¶è®¾ç½®å‚æ•° padding=SAME å³å¯ä½¿å¾—è¾“å…¥è¾“å‡ºå½¢çŠ¶ç›¸åŒ
# 3x3çš„filterè€ƒè™‘åˆ°äº†åƒç´ ä¸å…¶è·ç¦»ä¸º1ä»¥å†…çš„æ‰€æœ‰å…¶ä»–åƒç´ çš„å…³ç³»ï¼Œè€Œ5x5åˆ™æ˜¯è€ƒè™‘åƒç´ ä¸å…¶è·ç¦»ä¸º2ä»¥å†…çš„æ‰€æœ‰å…¶ä»–åƒç´ çš„å…³ç³»
# å·ç§¯æ ¸çš„ä½œç”¨ :
# 1) 1ä¸ªå·ç§¯å±‚å…·æœ‰ K ä¸ªå·ç§¯æ ¸ , æ¯ä¸ªå·ç§¯æ ¸æå–äº†è¾“å…¥å›¾ç‰‡çš„æŸä¸ªç»´åº¦ç‰¹å¾ å¹¶å¾—åˆ° K ä¸ªç‰¹å¾å›¾
# 2) ç‰¹å¾å›¾ä½œä¸ºè¾“å…¥å†è¢«å·ç§¯çš„è¯,å¯ä»¥åˆ™å¯ä»¥ç”±æ­¤æ¢æµ‹åˆ°"æ›´å¤§"çš„å½¢çŠ¶æ¦‚å¿µ å³å¤šä¸ªå·ç§¯å±‚ éšç€å·ç§¯ç¥ç»ç½‘ç»œå±‚æ•°çš„å¢åŠ ï¼Œç‰¹å¾æå–çš„è¶Šæ¥è¶Šå…·ä½“åŒ–

# å·ç§¯å±‚ç»“æŸåè¿›å…¥æ¿€åŠ±å±‚ æ¿€åŠ±å±‚çš„ä½œç”¨å¯ä»¥ç†è§£ä¸ºæŠŠå·ç§¯å±‚çš„ç»“æœåšéçº¿æ€§æ˜ å°„ f(Î£wixi+b)
# å¸¸è§çš„æ¿€æ´»å‡½æ•°æœ‰ Sigmoid Tanh Relu LeakyRelu ELU Maxout
# ä¸€èˆ¬ä¸è¦ç”¨sigmoidï¼Œé¦–å…ˆè¯•RELUï¼Œå› ä¸ºå¿«ï¼Œä½†è¦å°å¿ƒç‚¹ï¼Œå¦‚æœRELUå¤±æ•ˆï¼Œè¯·ç”¨Leaky ReLUï¼ŒæŸäº›æƒ…å†µä¸‹tanhå€’æ˜¯æœ‰ä¸é”™çš„ç»“æœ

# æ± åŒ–å±‚ : é™ä½äº†å„ä¸ªç‰¹å¾å›¾çš„ç»´åº¦ï¼Œä½†å¯ä»¥ä¿æŒå¤§åˆ†é‡è¦çš„ä¿¡æ¯ã€‚æ± åŒ–å±‚å¤¹åœ¨è¿ç»­çš„å·ç§¯å±‚ä¸­é—´ ï¼ˆè§ä¹¦P240é¡µï¼‰
# å‹ç¼©æ•°æ®å’Œå‚æ•°çš„é‡ï¼Œå‡å°è¿‡æ‹Ÿåˆï¼Œæ± åŒ–å±‚å¹¶æ²¡æœ‰å‚æ•°ï¼Œå®ƒåªä¸è¿‡æ˜¯æŠŠä¸Šå±‚ç»™å®ƒçš„ç»“æœåšäº†ä¸€ä¸ªä¸‹é‡‡æ ·ï¼ˆæ•°æ®å‹ç¼©ï¼‰
# poolingåœ¨ä¸åŒçš„é€šé“depthä¸Šæ˜¯åˆ†å¼€æ‰§è¡Œçš„ï¼Œä¹Ÿå°±æ˜¯depth=5çš„è¯ï¼Œpoolingè¿›è¡Œ5æ¬¡ï¼Œäº§ç”Ÿ5ä¸ªæ± åŒ–åçš„çŸ©é˜µ
# pool_size(2,2)è¡¨ç¤ºæ‰«æåŒºåŸŸå¤§å°2Ã—2 , strides=(1,1)è¡¨ç¤ºæ°´å¹³å’Œç«–ç›´è·³è·ƒæ­¥é•¿
# æ± åŒ–æ“ä½œæ˜¯åˆ†å¼€åº”ç”¨åˆ°å„ä¸ªé€šé“ç‰¹å¾å›¾çš„ï¼Œæˆ‘ä»¬å¯ä»¥ä»64ä¸ªè¾“å…¥å›¾ä¸­å¾—åˆ°64ä¸ªè¾“å‡ºå›¾ å³512*512*64 (pooling)----> 128*128*64
# 1) æœ€å¤§å€¼æ± åŒ– : Max pooling å®šä¹‰ä¸€ä¸ªç©ºé—´é‚»åŸŸå¦‚ 2Ã—2 çš„çŸ©é˜µé€‰æ‹©ä¸€ä¸ªæœ€å¤§çš„å…ƒç´ ä½œä¸ºè¢«é‡‡æ ·çš„ç‚¹ é‚£ä¹ˆ4Ã—4çš„å›¾ç‰‡å¯ä»¥å‹ç¼©åˆ° 2Ã—2
# 2) å¹³å‡å€¼æ± åŒ– :ã€€Average poolingã€€å–å¹³å‡å€¼

# å‚æ•°æ ‡å‡†åŒ–å±‚(BNå±‚)ï¼šBatch Nomalization ä¸€èˆ¬å †å å±‚é¡ºåºä¸º Conv-BN-ReLU-Pooling å…ˆå·ç§¯å†æ ‡å‡†åŒ–å†æ¿€æ´»
# BN å±‚çš„æå‡ºï¼Œä½¿å¾—ç½‘ç»œçš„è¶…å‚æ•°çš„è®¾å®šæ›´åŠ è‡ªç”±ï¼Œæ¯”å¦‚æ›´å¤§çš„å­¦ä¹ ç‡ï¼Œæ›´éšæ„çš„ç½‘ç»œåˆå§‹åŒ–ç­‰ åŒæ—¶ç½‘ç»œçš„æ”¶æ•›é€Ÿåº¦æ›´å¿«ï¼Œæ€§èƒ½ä¹Ÿæ›´å¥½
# ç½‘ç»œå±‚è¾“å…¥ğ‘¥åˆ†å¸ƒç›¸è¿‘ï¼Œå¹¶ä¸”åˆ†å¸ƒåœ¨è¾ƒå°èŒƒå›´å†…æ—¶(å¦‚ 0 é™„è¿‘)ï¼Œæ›´æœ‰åˆ©äºå‡½æ•°çš„ä¼˜åŒ–
# å½’ä¸€åŒ–å…¬å¼(x-Î¼)/sqrt(Ïƒ^2+Îµ) Îµæ˜¯ä¸ºäº†é˜²æ­¢é™¤0é”™è¯¯
# scale and shift æŠ€å·§ å¯¹å½’ä¸€åŒ–çš„æ•°æ®å†è¿›è¡Œç¼©æ”¾å’Œå¹³ç§» x_norm = x_norm *Î³ +Î²
# ğ›¾ ğ›½å‚æ•°å‡ç”±åå‘ä¼ æ’­ç®—æ³•è‡ªåŠ¨ä¼˜åŒ–ï¼Œå®ç°ç½‘ç»œå±‚â€œæŒ‰éœ€â€ç¼©æ”¾å¹³ç§»æ•°æ®çš„åˆ†å¸ƒçš„ç›®çš„
# BNå±‚å¯¹è¾“å…¥çš„x_trainå½’ä¸€åŒ–å¾—åˆ°x~_trainï¼Œx~_train = (x_train-Î¼B)/ÏƒB * Î³ +Î² ä½¿ç”¨çš„æ˜¯å½“å‰è®­ç»ƒbatchçš„å¹³å‡å€¼å’Œæ–¹å·®
# ç„¶åå¯¹å…¨å±€è®­ç»ƒæ•°æ®çš„Î¼r å’ŒÏƒræŒ‰ç…§å…¬å¼ M*Î¼r+(1-M)Î¼B      M*Ïƒr^2 + (1-M)Ïƒr^2  è¿›è¡Œæ›´æ–°
# å³M=0æ—¶ä½¿ç”¨å½“å‰æœ€æ–°çš„batchç»Ÿè®¡å€¼ï¼ŒM=1å¿½ç•¥å½“å‰batchï¼Œä¸€èˆ¬M=0.99
# æµ‹è¯•é˜¶æ®µç›´æ¥ä½¿ç”¨è®­ç»ƒæ•°æ®çš„å…¨å±€å‚æ•°ä¸æ›´æ–° x~_test = (x_test-Î¼r)/Ïƒr * Î³ +Î²
# è®­ç»ƒæ¨¡å¼ä¸‹çš„åå‘æ›´æ–°é˜¶æ®µï¼Œåå‘ä¼ æ’­ç®—æ³•æ ¹æ®æŸå¤±Læ±‚è§£æ¢¯åº¦âˆ‚L/Î³ å’Œ âˆ‚L/Î²
# ç»Ÿè®¡æ—¶æŒ‰ç…§é€šé“è¿›è¡Œè®¡ç®—ï¼Œcä¸ªé€šé“åˆ™æœ‰cä¸ªå‡å€¼
# Layer Normï¼šç»Ÿè®¡æ¯ä¸ªæ ·æœ¬çš„æ‰€æœ‰ç‰¹å¾çš„å‡å€¼å’Œæ–¹å·®
# Instance Normï¼šç»Ÿè®¡æ¯ä¸ªæ ·æœ¬çš„æ¯ä¸ªé€šé“ä¸Šç‰¹å¾çš„å‡å€¼å’Œæ–¹å·®
# Group Normï¼šå°† c é€šé“åˆ†æˆè‹¥å¹²ç»„ï¼Œç»Ÿè®¡æ¯ä¸ªæ ·æœ¬çš„é€šé“ç»„å†…çš„ç‰¹å¾å‡å€¼å’Œæ–¹å·®

# ç©ºæ´å·ç§¯åœ¨ä¸å¢åŠ ç½‘ç»œå‚æ•°çš„æ¡ä»¶ä¸‹ï¼Œæä¾›äº†æ›´å¤§çš„æ„Ÿå—é‡çª—å£
# layers.Conv2D()ç±»çš„ dilation_rate å‚æ•°æ¥é€‰æ‹©ä½¿ç”¨æ™®é€šå·ç§¯è¿˜æ˜¯ç©ºæ´å·ç§¯

# è½¬ç½®å·ç§¯ï¼šé€šè¿‡åœ¨è¾“å…¥ä¹‹é—´å¡«å……å¤§é‡çš„ padding æ¥å®ç°è¾“å‡ºé«˜å®½å¤§äºè¾“å…¥é«˜å®½çš„æ•ˆæœï¼Œä»è€Œå®ç°å‘ä¸Šé‡‡æ ·çš„ç›®çš„
# tf.nn.conv2d_transposeï¼Œlayers.Conv2DTransposeï¼Œpadding='VALID'æˆ–"SAME"ä¸æ”¯æŒè‡ªå®šä¹‰


# å…¨è¿æ¥å±‚ : é€šå¸¸åœ¨ç¥ç»ç½‘ç»œçš„å°¾éƒ¨ , æ­¤æ—¶å·²ç»æå¤§é™ä½äº†è¾“å…¥æ•°æ®çš„ç»´åº¦ã€æ€»å‚æ•°ä¸ªæ•°
# é€šå¸¸å·ç§¯ç½‘ç»œçš„æœ€åä¼šå°†æœ«ç«¯å¾—åˆ°çš„é•¿æ–¹ä½“å¹³æ‘Šæˆä¸€ä¸ªé•¿é•¿çš„å‘é‡ï¼Œå¹¶é€å…¥å…¨è¿æ¥å±‚é…åˆè¾“å‡ºå±‚è¿›è¡Œåˆ†ç±»
# å¯ä»¥è®¤ä¸ºå…¨è¿æ¥å±‚ä¹‹é—´çš„åœ¨åšç‰¹å¾æå–ï¼Œè€Œå…¨è¿æ¥å±‚åœ¨åšåˆ†ç±»ï¼Œè¿™å°±æ˜¯å·ç§¯ç¥ç»ç½‘ç»œçš„æ ¸å¿ƒ

# 2012 AlexNet-->2014 VGG-->GoogLeNet 2015 -->ResNet 2015 --> DenseNet 2016

# å¸¸è§çš„CNNå·ç§¯ç½‘ç»œæµç¨‹
# input-->conv2d-->max_pooling-->conv2d_1-->max_pooling_1-->reshape-->dense-->dropout-->dense_1-->output
# examples :
# 1) é¡ºåºå¼API : æœ€å¸¸è§çš„ä¸€ç±»æ¨¡å‹æ˜¯ä¸€ç»„å±‚çš„å †å 
n_classes = 10
(train_x,train_y),(test_x,test_y) = fashion_mnist .load_data()
train_x = train_x / 255. * 2 -1
test_x = test_x / 255. * 2 -1  # [-1,1]
epochs = 10
batch_size = 32
per_epoch_iters = int(train_x.shape[0] / batch_size)
model = tf.keras.Sequential([
    # filters = 32 å·ç§¯æ ¸çš„æ•°é‡ ä¹Ÿå°±æ˜¯ä¸‹ä¸€å±‚çš„è¾“å‡ºä¸ªæ•° kernel_size å·ç§¯æ ¸çš„é«˜åº¦å’Œå®½åº¦
    # ç¬¬ä¸€å±‚æƒé‡å‚æ•°ä¸º filters * kernel_size^2 * D1(è¾“å…¥å±‚çš„æ·±åº¦) = 32*5*5*1(ç°åº¦å›¾)
    # ç¬¬ä¸€å±‚çš„åç½®å‚æ•°ä¸ºfiltersä¸ªåç½® 32
    # parms_1 = 840 + 32 = 832
    # è¾“å…¥å°ºå¯¸ :ã€€28 * 28
    # output_shape = (input_shape - kernel_size + 1) / strides) = (28 - 5) + 1 = 24 * 24
    # (None, 24, 24, 32) è¾“å‡ºæ·±åº¦ = filters = 32
    tf.keras.layers.Conv2D(filters=32 ,kernel_size= (5,5),strides=(1,1),activation=tf.nn.relu,input_shape=(28,28,1)),

    # æ± åŒ–å±‚ä¸å½±å“å‚æ•°ä¸ªæ•° åªæ˜¯è¿›è¡Œæ•°æ®å‹ç¼©é‡‡æ · (None, 12, 12, 32)
    # è¾“å…¥å°ºå¯¸ = 24 * 24
    # poolingåœ¨ä¸åŒçš„depthä¸Šæ˜¯åˆ†å¼€æ‰§è¡Œçš„ æ•… è¾“å‡ºæ·±åº¦ = 32ä¸å˜
    # pool_size=(2, 2)è¡¨ç¤ºæ¯4ä¸ªåƒç´ å¾—åˆ°ä¸€ä¸ªå€¼,æ¯2è¡Œç¼©æˆ1è¡Œ
    # æ•…è¾“å‡ºå°ºå¯¸ä¸º 12 * 12
    tf.keras.layers.MaxPooling2D(pool_size=(2, 2),strides=(2,2), padding='valid'),

    # ç»§ç»­å·ç§¯
    # ç¬¬äºŒå±‚æƒé‡å‚æ•° = 64*3*3*32 = 18432  + 64ä¸ªåç½®å‚æ•° = 18496
    # è¾“å…¥å°ºå¯¸  = 12 * 12
    # è¾“å‡ºå°ºå¯¸ = 12 - 3 + 1 = 10
    # è¾“å‡ºæ·±åº¦ = 64
    # (None, 10, 10, 64)
    tf.keras.layers.Conv2D(filters=64,kernel_size=(3,3),activation=tf.nn.relu),

    # è¾“å…¥å°ºå¯¸ = 10 * 10
    # è¾“å‡ºæ·±åº¦ = 64 ä¸å˜
    # è¾“å‡ºå°ºå¯¸ = 10 / 2 = 5 * 5
    # (None, 5, 5, 64)
    tf.keras.layers.MaxPooling2D(pool_size=(2, 2),strides=(2,2), padding='valid'),

    # å±•å¹³å±‚å‘é‡ç”¨äºè¡”æ¥å…¨è¿æ¥å±‚Dense
    # 5*5*64 = 1600
    tf.keras.layers.Flatten(),

    # ç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚æŒ‡å®š1024ä¸ªç¥ç»å…ƒ 1600*1024 ä¸ªæƒé‡å‚æ•° + 1024 ä¸ªåç½®å‚æ•° = 1639424
    tf.keras.layers.Dense(units=1024,activation=tf.nn.relu),

    # å¼•å…¥Dropouté˜²æ­¢è¿‡æ‹Ÿåˆ
    # è®­ç»ƒæœŸé—´çš„æ¯ä¸€æ­¥ä»¥ rate çš„é¢‘ç‡éšæœºå°†è¾“å…¥å•å…ƒè®¾ç½®ä¸º 0ï¼Œè¿™æœ‰åŠ©äºé˜²æ­¢è¿‡æ‹Ÿåˆã€‚
    # æœªè®¾ç½®ä¸º 0 çš„è¾“å…¥æŒ‰ 1ï¼ˆ1 - rateï¼‰æ”¾å¤§ï¼Œä»¥ä¾¿æ‰€æœ‰è¾“å…¥çš„æ€»å’Œä¿æŒä¸å˜
    tf.keras.layers.Dropout(rate=0.5),

    # æœ€åä¸€å±‚,åˆ†ç±»å±‚çš„å…¨è¿æ¥å±‚ å‚æ•° = 1024*10+10=10250
    tf.keras.layers.Dense(n_classes)
])
# tf.expand_dimså¯ä»¥1:1æ›¿ä»£np.expand_dims å³å¯¹train_xæ·»åŠ ä¸€ä¸ªç»´åº¦ ä½†æ˜¯åŒºåˆ«åœ¨äºç”Ÿæˆçš„æ˜¯tf.Tensorå¯¹è±¡
# ç”±äºmodel.compileæ–¹æ³•éœ€è¦np.arrayå¯¹è±¡ æ‰€ä»¥ç»§ç»­ä½¿ç”¨äº†numpyæ–¹æ³•
# æ¯ä¸ªtf.Tensoréƒ½æœ‰å¯¹åº”çš„Tensorå¯¹è±¡åŒ…å«çš„Numpyå€¼
# shape=(60000, 28, 28, 1), dtype=float64) ----> shape=(60000, 28, 28, 1, dtype=float64)
train_x = tf.expand_dims(train_x,-1).numpy()
# å¯¹è¾“å…¥çš„å¼ é‡å¢åŠ ç»´åº¦,è‡³äºæ•°å­—æ²¡æœ‰å¤ªå¤§æ„ä¹‰ axisæŒ‡å®šæ’å…¥çš„ç»´åº¦æ‰€åœ¨ä½ç½®
test_x  = tf.expand_dims(test_x, -1).numpy()
model.summary()
model .compile(optimizer= tf.keras.optimizers .Adam(learning_rate=1e-5),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
# N=60000,batch_size = 32 , per_epoch_iters = 1875 , epochs = 10, total_iters = 18750
model.fit(train_x,train_y,epochs=epochs,batch_size=None)

# N_ = 10000 batch_size = 32 per_epoch_iters = 313
model.evaluate(test_x,test_y)
# 313/313 [==============================] - 5s 15ms/step - loss: 0.7884 - accuracy: 0.7809

#%%
# 2) å‡½æ•°å¼API : å¯ä»¥å®šä¹‰å¤æ‚çš„æ‹“æ‰‘æ— éœ€è€ƒè™‘é¡ºåºå±‚çš„é™åˆ¶ å¯ä»¥å®šä¹‰å¤šè¾“å…¥å¤šè¾“å‡ºçš„æ¨¡å‹
# å±•ç¤ºä¸€ä¸ªä½¿ç”¨å‡½æ•°å‹æ¥å£çš„Kerasæ¨¡å‹ è¯¥ç¥ç»ç½‘ç»œæ¥æ”¶100ç»´è¾“å…¥ è¾“å‡ºä¸€ä¸ªæ•°
# ç½‘ç»œå±‚çš„å®ä¾‹æ˜¯å¯è°ƒç”¨çš„ï¼Œå®ƒä»¥å¼ é‡ä¸ºå‚æ•°ï¼Œå¹¶ä¸”è¿”å›ä¸€ä¸ªå¼ é‡
# è¾“å…¥å’Œè¾“å‡ºå‡ä¸ºå¼ é‡ï¼Œå®ƒä»¬éƒ½å¯ä»¥ç”¨æ¥å®šä¹‰ä¸€ä¸ªæ¨¡å‹ï¼ˆModelï¼‰
# åˆ©ç”¨å‡½æ•°å¼ APIï¼Œå¯ä»¥è½»æ˜“åœ°é‡ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ï¼šå¯ä»¥å°†ä»»ä½•æ¨¡å‹çœ‹ä½œæ˜¯ä¸€ä¸ªå±‚ï¼Œç„¶åé€šè¿‡ä¼ é€’ä¸€ä¸ªå¼ é‡æ¥è°ƒç”¨å®ƒ
# æ³¨æ„ï¼Œåœ¨è°ƒç”¨æ¨¡å‹æ—¶ï¼Œä¸ä»…é‡ç”¨æ¨¡å‹çš„ç»“æ„ï¼Œè¿˜é‡ç”¨äº†å®ƒçš„æƒé‡
# è¿™éƒ¨åˆ†è¿”å›ä¸€ä¸ªå¼ é‡
input_shape = (100,)
# å±‚çš„å®ä¾‹æ˜¯å¯è°ƒç”¨çš„ï¼Œå®ƒä»¥å¼ é‡ä¸ºå‚æ•°ï¼Œå¹¶ä¸”è¿”å›ä¸€ä¸ªå¼ é‡
inputs = tf.keras.layers.Input(input_shape)  # ç”¨äºå®ä¾‹åŒ– Keras å¼ é‡
# Keraså¼ é‡çš„å±æ€§ä½¿æˆ‘ä»¬èƒ½å¤Ÿä»…é€šè¿‡äº†è§£æ¨¡å‹çš„è¾“å…¥å’Œè¾“å‡ºæ¥æ„å»º Keras æ¨¡å‹
# å‡è®¾å­˜åœ¨å¼ é‡aã€bã€c é‚£ä¹ˆ model = Model(input=[a, b], output=c)
net = tf.keras.layers.Dense(units=64,activation=tf.nn.relu,name='fc1')(inputs) # 64 * 100 + 64
net = tf.keras.layers.Dense(units=64,activation=tf.nn.relu,name='fc2')(net) # 64 * 64 + 64
net = tf.keras.layers.Dense(units=1,activation=tf.nn.relu,name='G')(net)# 64 * 1 + 1
# è¿™éƒ¨åˆ†åˆ›å»ºäº†ä¸€ä¸ªåŒ…å«è¾“å…¥å±‚å’Œä¸‰ä¸ªå…¨è¿æ¥å±‚çš„æ¨¡å‹
model = tf.keras.Model(inputs=inputs,outputs=net)
model.compile(optimizer='rmsprop',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
model.summary()
#%%
# 3) å­ç±»æ–¹æ³• : é¢å‘å¯¹è±¡æ›´çµæ´»ä½†æ˜¯éš¾ä»¥è°ƒè¯•
class Generator(tf.keras.Model) :

    def __init__(self):
        # å¯¹ç»§æ‰¿è‡ªçˆ¶ç±»çš„å±æ€§è¿›è¡Œåˆå§‹åŒ–ã€‚è€Œä¸”æ˜¯ç”¨çˆ¶ç±»çš„åˆå§‹åŒ–æ–¹æ³•æ¥åˆå§‹åŒ–ç»§æ‰¿çš„å±æ€§
        # å­ç±»ç»§æ‰¿äº†çˆ¶ç±»çš„æ‰€æœ‰å±æ€§å’Œæ–¹æ³•ï¼Œçˆ¶ç±»å±æ€§è‡ªç„¶ä¼šç”¨çˆ¶ç±»æ–¹æ³•æ¥è¿›è¡Œåˆå§‹åŒ–
        # åˆå§‹åŒ–çš„é€»è¾‘ä¸çˆ¶ç±»çš„ä¸åŒï¼Œä¸ä½¿ç”¨çˆ¶ç±»çš„æ–¹æ³•ï¼Œè‡ªå·±é‡æ–°åˆå§‹åŒ–ä¹Ÿæ˜¯å¯ä»¥çš„
        super(Generator,self).__init__()
        self.dense_1 = tf.keras.layers.Dense(
            units=64,activation=tf.nn.relu,name='fc1'
        )
        self.dense_2 = tf.keras.layers.Dense(
            units=64,activation=tf.nn.relu,name='fc2'
        )
        self.output_1 = tf.keras.layers.Dense(units=1,name='G')

    def call(self , inputs):
        net = self.dense_1(inputs)
        net = self.dense_2(net)
        net = self.output_1(net)
        return net
# %%
# ã€12ã€‘ å¸¸è§çš„OPè¿ç®—
# 1) æ ‡é‡è¿ç®— addã€subã€divã€mulã€logã€greaterã€lessã€equal
# 2) å‘é‡è¿ç®— concatã€sliceã€constantã€rankã€shapeã€shuffle
# 3) çŸ©é˜µè¿ç®— matmulã€matrixinverseã€matrixdateminant
# 4) å¸¦çŠ¶æ€çš„è¿ç®— variable ã€assign ã€assignadd
# 5) ç¥ç»ç½‘ç»œç»„ä»¶ softmaxã€sigmoidã€reluã€convolutionã€max_pool
# 6) å­˜å‚¨ã€æ¢å¤ save ã€restore
# 7) é˜Ÿåˆ—å’ŒåŒæ­¥è¿ç®— Enqueueã€Dequeueã€MutexAcquireã€MutexRelease
# 8) æ§åˆ¶æµ Mergeã€Switchã€Leaveã€NextIteration
#%%
# ã€13ã€‘ tf.data.Datasetæ•°æ®é›†å¯¹è±¡
# è¯¥å¯¹è±¡å°†ä¸€ä¸ªè¾“å…¥æµæ°´çº¿å˜ä¸ºä¸€ç³»åˆ—å…ƒç´ ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«äº†1ä¸ªæˆ–å¤šä¸ªtf.Tensorå¯¹è±¡
# tf.data.Datasetæœ¬èº«æ˜¯å¯è¿­ä»£çš„ï¼Œä½†ä¸æ˜¯è¿­ä»£å™¨ ä¸€èˆ¬éœ€è¦next(iter)é…åˆä½¿ç”¨æ¥æšä¸¾å…ƒç´  æˆ–è€…ä½¿ç”¨forå¾ªç¯


# Raw data ---> .map() ---> .cache() ---> .shuffle() ---> .batch() ---> .prefetch ---> Model input
# CPUæ‰§è¡Œç”Ÿäº§æ•°æ® CPU/GPU/TPUæ¶ˆè´¹æ•°æ® éœ€è¦å¹³è¡¡ç›®æ ‡è®¾å¤‡å’Œç”Ÿäº§è®¾å¤‡çš„å¤„ç†é€Ÿåº¦ ç”¨åˆ°å¹¶è¡Œè®¡ç®—æŠ€æœ¯
# å¹¶è¡Œè®¡ç®—å¯ä»¥æé«˜ç›®æ ‡è®¾å¤‡çš„èµ„æºåˆ©ç”¨ç‡ åå°ä¸æ–­å·¥ä½œäº§ç”Ÿä¸‹ä¸€è½®è¿­ä»£æ¶ˆè´¹è€…éœ€è¦çš„æ•°æ®
# .cache()æ˜¯å°†æ•°æ®ç¼“å­˜åˆ°æ•°æ®å†…å­˜ä¸­ å¯ä»¥åŠ é€Ÿæ¥ä¸‹æ¥çš„è¿ç®—
# .repeat()ç›®çš„æ˜¯åœ¨å¤šè½®è®­ç»ƒä¸­å°†è®­ç»ƒé›†çš„å…¨éƒ¨æ•°æ®å–‚ç»™æ¨¡å‹ æ‰€ä»¥éœ€è¦é‡å¤
# é¢„å–æ–¹æ³•.prefetch(n)ç”¨æ¥åº”ç”¨ç”Ÿäº§è€…å’Œæ¶ˆè´¹è€…å·¥ä½œé‡å çš„å˜æ¢ nçš„å•ä½æ˜¯å¤šå°‘ä¸ªbatch è¡¨ç¤ºä¸€æ¬¡è¿­ä»£æ¶ˆè´¹çš„æ ·æœ¬æ•°

# åˆ›å»ºæ•°æ®é›†å¯¹è±¡å–å†³äºæ•°æ®æºçš„æ ¼å¼ï¼Œä¸»è¦æœ‰ä»¥ä¸‹äº”ç§
# 1) ä»å†…å­˜ä¸­çš„å¼ é‡è·å– : tf.data.Dataset.from_tensors æˆ–è€… tf.data.Dataset.from_tensor_slices
# è¿™äº›å¼ é‡å¯ä»¥æ˜¯Numpyæ•°ç»„æˆ–è€…tf.Tensorå¯¹è±¡
# ä½œç”¨æ˜¯æŠŠç»™å®šçš„å…ƒç»„ã€åˆ—è¡¨å’Œå¼ é‡ç­‰æ•°æ®è¿›è¡Œç‰¹å¾åˆ‡ç‰‡,èŒƒå›´ä»æœ€å¤–å±‚ç»´åº¦å¼€å§‹
# å¦‚æœæœ‰å¤šä¸ªç‰¹å¾è¿›è¡Œç»„åˆï¼Œé‚£ä¹ˆä¸€æ¬¡åˆ‡ç‰‡æ˜¯æŠŠæ¯ä¸ªç»„åˆæœ€å¤–ç»´åº¦çš„æ•°æ®åˆ‡å¼€ï¼Œåˆ†æˆå¤šç»„
dataset = tf.data.Dataset.from_tensor_slices(
    # tf.random.uniform(shape=[])å¾—åˆ°å¼ é‡
    {"a": tf.random.uniform(shape=[4]) , # ä»å‡åŒ€åˆ†å¸ƒä¸­è¾“å‡ºéšæœºå€¼ ,é»˜è®¤èŒƒå›´æ˜¯ [0, 1)
     "b": tf.random.uniform(shape=[4,100],maxval=100,dtype=tf.int32 )}
) # å¾—åˆ°TensorSliceDatasetæ•°æ®é›† å‹ç¼©äº†4Ã—100çš„å­—å…¸
list(dataset.as_numpy_iterator()) # å°†æ•°æ®é›†çš„æ‰€æœ‰å…ƒç´ è½¬æ¢ä¸º numpy å¾—åˆ°ä¸€ä¸ªè¿­ä»£å™¨è¾“å‡º listæ˜¯è½¬æ¢è¾“å‡º
for value in dataset : # è¡¨ç¤ºdatasetçš„æ•°æ®å­é›†
    print(value["a"]) # æ‰“å°å­é›†å…³é”®è¯"a"çš„æ•°ç»„
#%%
# 2) ä»ä¸€ä¸ªPythonç”Ÿæˆå™¨/è¿­ä»£å™¨è·å– tf.data.Dataset.from_generator
# å…³äºç”Ÿæˆå™¨çš„ç”Ÿæˆæ–¹æ³•ä¸»è¦æ˜¯ä¸¤ç§ï¼Œä¸€ç§æ˜¯ä½¿ç”¨åˆ—è¡¨äº§ç”Ÿï¼Œä¸€ç§æ˜¯å€ŸåŠ©å‡½æ•°+yieldäº§ç”Ÿ

# 1. g = (è¡¨è¾¾å¼ for å˜é‡ in åˆ—è¡¨)
generator = (x*x  for x in range(10))
# ç›¸å¯¹äºlist = [x*x  for x in range(20) ]å¥½å¤„åœ¨äºä¸éœ€è¦åˆ›å»ºå®Œæ•´çš„æ•°æ®åˆ—è¡¨ èŠ‚çœå†…å­˜
print(generator.__next__()) # æˆ–è€…
print(next(generator)) # è¯­å¥å‡ºç°å¤šå°‘æ¬¡æ‰ä¼šè¿­ä»£å¤šå°‘æ¬¡ æ‰€ä»¥éœ€è¦é…åˆå¾ªç¯
# æˆ–è€…å€ŸåŠ©list(map(func,iter))è¾“å‡ºå…ƒç´ 
def func(value):
    return value * 2
print(list(map(func,generator )))

# 2. å‡½æ•° + yield åˆ©ç”¨å‡½æ•°å¾—åˆ°ç”Ÿæˆå™¨
def func_1():
    n = 0
    while 1:
        n += 1
        yield n
g = func_1()  # æ­¤å‡½æ•°ä¸ºç”Ÿæˆå™¨å¯¹è±¡
print(next(g))
print(next(g))

def noise():
    # å®šä¹‰ä¸€ä¸ªäº§ç”Ÿæ— é™æ•°é‡çš„éšæœºå‘é‡çš„æ•°æ®é›† æ¯ä¸ªå‘é‡æœ‰100ä¸ªå…ƒç´ 
    while 1 :
        yield  tf.random.uniform((100,))
dataset = tf.data.Dataset.from_generator(noise,(tf.float32)) # å‚æ•°ç±»å‹å¿…é¡»æŒ‡å®š
# <FlatMapDataset shapes: <unknown>, types: tf.float32>
# tf.data.Dataset.from_generatorç”¨æ³•ç±»ä¼¼äºmap,å¾—åˆ°çš„æ•°æ®ç±»å‹æ˜¯FlatMapDataset
# é“¾å¼è°ƒç”¨åˆ›å»ºæ–°çš„æ•°æ®é›†å¯¹è±¡ï¼Œmapæ–¹æ³•å…è®¸å°†ä¸€ä¸ªå‡½æ•°åº”ç”¨åœ¨è¾“å…¥æ•°æ®é›†çš„æ¯ä¸€ä¸ªå…ƒç´ ä¸Šï¼Œç”Ÿæˆä¸€ä¸ªæ–°çš„å˜æ¢è¿‡çš„æ•°æ®é›†
dataset = dataset.map(map_func=lambda x:x+10).shuffle(buffer_size=10).batch(batch_size=32)
# <BatchDataset shapes: <unknown>, types: tf.float32>
#%%
# 3) ä»æ¨¡å¼åŒ¹é…çš„æ–‡ä»¶ tf.data.Dataset.list_files
# 4) å¤„ç†TFRecordæ–‡ä»¶ tf.data.TFRecordDataset
# TFRecordæ ¼å¼æ˜¯ä¸€ä¸ªäºŒè¿›åˆ¶çš„ä¸è¯­è¨€æ— å…³çš„æ ¼å¼ã€ä½¿ç”¨protobufå®šä¹‰ç”¨æ¥å­˜å‚¨ä¸€ç³»åˆ—äºŒè¿›åˆ¶è®°å½•
# Tensorflowå¯ä»¥è¯»å†™ä¸€ç³»åˆ—tf.Exampleæ¶ˆæ¯ç»„æˆçš„TFRecordæ–‡ä»¶
# tf.Exampleä½œä¸ºæ¶ˆæ¯ç±»å‹ å­˜åœ¨å­—å…¸æ˜ å°„ key(ç‰¹å¾åç§°)-->value(äºŒè¿›åˆ¶è¡¨ç¤º)
# è¯»å–çš„å›¾åƒä¸æ˜¯å‹ç¼©æ ¼å¼è€Œæ˜¯äºŒè¿›åˆ¶æ ¼å¼ è¯»å–é€Ÿåº¦åŠ å¿«æ— éœ€è§£ç  ä½†è€—è´¹ç¡¬ç›˜ç©ºé—´
import tensorflow_datasets # tfds-->åŸºäºTFRecordæ–‡ä»¶è§„èŒƒçš„é«˜çº§API
# 5) å¤„ç†æ–‡æœ¬æ–‡ä»¶ tf.data.TextLineDataset ä»¥è¡Œä¸ºå•ä½è¯»å–

# 6) æ•°æ®å¢å¼º/æ‰©å……æŠ€æœ¯ tf.imageåŒ…æä¾›çš„API
# å¯ä»¥å…ˆå®šä¹‰ä¸ªå¢å¼ºå‡½æ•°å†ä½¿ç”¨dataset.mapæ–¹æ³•è¿›è¡Œå˜æ¢
# è¿˜å¯ä½¿ç”¨ç¬¬ä¸‰æ–¹åº“imgaugã€albumentationså¯¹æ•°æ®è¿›è¡Œæ‰©å……
def convert_image(image):
    # éšæœºæ°´å¹³ç¿»è½¬å›¾åƒï¼ˆä»å·¦åˆ°å³ï¼‰å®½åº¦
    image = tf.image.random_flip_left_right(image)
    # ä»¥äºŒåˆ†ä¹‹ä¸€çš„æœºä¼šï¼Œè¾“å‡ºæ²¿ç¬¬ä¸€ä¸ªç»´åº¦ç¿»è½¬çš„å›¾åƒçš„å†…å®¹ï¼Œå³é«˜åº¦
    image = tf.image.random_flip_up_down(image)
    # é€šè¿‡éšæœºå› å­è°ƒæ•´å›¾åƒçš„äº®åº¦
    image = tf.image.random_brightness(image,max_delta=0.1)
    return image

# example : å®šä¹‰çš„å‡½æ•°å¯ç«‹å³è¿”å›tf.data.Datasetå¯¹è±¡
def train_dataset(batch_size=32,num_epochs=1):
    (train_x,train_y),(test_x,test_y) = fashion_mnist.load_data()
    input_x , input_y = train_x , train_y
    def scale_fn(image,label) :
       # å½’ä¸€åŒ–å›¾åƒæ ¼å¼åˆ°[-1,1]
        return (tf.image.convert_image_dtype(image,tf.float32) - 0.5 ) *2.0 ,label
    dataset = tf.data.Dataset.from_tensor_slices(
        (tf.expand_dims(input_x,-1) , tf.expand_dims(input_y,-1))
    ).map(scale_fn)

    dataset = dataset.cache().repeat(num_epochs)
    dataset = dataset.shuffle(batch_size)

    return dataset.batch(batch_size).prefetch(1) # æŠ“å–çš„æ ·æœ¬é¢„å–ä¸€ä¸ªè¿­ä»£æ•°é‡

dataset_ = train_dataset()

#%% ã€14ã€‘ æ ‡å‡†çš„ETLè¿‡ç¨‹
# æå–Extract è½¬æ¢Transform è½½å…¥Load
# é€šè¿‡å®šä¹‰get_input_fnå‡½æ•°å°†ETè¿‡ç¨‹å°è£… input_fnå‡½æ•°è¿”å›featureså’Œlabelså¯¹è±¡
# æ¨¡å‹ã€æ•°æ®é›†å¯ä»¥å¤„äºä¸åŒçš„æ¨¡å¼ï¼Œè¿™ä¸ªæ¨¡å¼å†³å®šå¤„äºæ•°æ®æµæ°´çº¿çš„é˜¶æ®µ
# å¯ä»¥é€šè¿‡æšä¸¾enumç±»å‹tf.estimator.ModeKeyså®ç° åŒ…æ‹¬Trainã€Evalã€Predictï¼Œå³è®­ç»ƒã€è¯„ä¼°ã€æ¨ç†æ¨¡å¼
# å®šä¹‰è·å–è¾“å…¥æµæ°´çº¿å‡½æ•°get_input_fn
def get_input_fn(mode,batch_size=32,num_epochs=1):
    '''
    :param mode: æ ¹æ®tf.estimator.ModeKeysçš„æ¨¡å¼è·å–è¾“å…¥å‡½æ•°
    :param batch_size: ä¸€æ¬¡è¿­ä»£æŠ“å–çš„æ ·æœ¬æ•°
    :param num_epochs: æ¨¡å‹éå†è®­ç»ƒé›†çš„æ¬¡æ•°
    :return: è¿”å›è¾“å…¥å‡½æ•°input_fn  è®­ç»ƒæ¨¡å¼æ—¶æ‰§è¡Œéšæœºé‡å¤å¦åˆ™ä¸æ‰§è¡Œ
             input_fnçš„è¿”å›å€¼ä¸ºfeatureså’Œlabels
    '''
    (train_x,train_y),(test_x,test_y) = fashion_mnist.load_data()
    half = test_x.shape[0] // 2
    if mode == tf.estimator.ModeKeys.TRAIN :
        input_x ,input_y = train_x , train_y
        train = True
    elif mode == tf.estimator.ModeKeys.EVAL:
        input_x , input_y = test_x[:half] ,test_y[:half]
        train = False
    elif mode == tf.estimator.ModeKeys.PREDICT :
        input_x, input_y = test_x[half:-1], test_y[half:-1]
        train = False
    else:
        raise ValueError("tf.estimator.ModeKeys required!")

    def scale_fn(image,label) :
        return ( 2 * tf.image.convert_image_dtype(image,tf.float32) - 1 ,
                 # tf.cast()å‡½æ•°çš„ä½œç”¨æ˜¯æ‰§è¡Œ tensorflow ä¸­å¼ é‡æ•°æ®ç±»å‹è½¬æ¢
                 # å°†labelçš„Tensoræˆ–SparseTensoræˆ–IndexedSlicesè½¬æ¢æˆdtypeç±»å‹
                 tf.cast(label,tf.int32))
    def input_fn():
        # 1ã€å°†å®Œæ•´çš„è®­ç»ƒé›†å¼ é‡æ•°æ®é›†åˆ‡ç‰‡è¿”å›DataSetå¯¹è±¡ ä½¿ç”¨mapå½’ä¸€åŒ–
        dataset = tf.data.Dataset.from_tensor_slices((tf.expand_dims(input_x,-1),
                                                     tf.expand_dims(input_y,-1))).map(scale_fn)
        # 2ã€å½“æŒ‡å®šä¸ºæ¨¡å‹è®­ç»ƒé˜¶æ®µæ—¶è¿›å…¥æ‰§è¡Œéšæœºæ“ä½œ
        if train :
            dataset = dataset.shuffle(10).repeat(count=num_epochs)
            # é‡å¤æ­¤æ•°æ®é›†ï¼Œå› æ­¤æ¯ä¸ªåŸå§‹å€¼éƒ½ä¼šè¢«çœ‹åˆ°countæ¬¡ å³ä¸€ä¸ªå‘¨æœŸ
        # 3ã€å¦åˆ™ç›´æ¥é¢„å–1ä¸ªbatchçš„æ•°æ®é›†å¹¶è¿”å›
        return dataset.batch(batch_size).prefetch(1) # é¢„å–1ä¸ªbatchçš„æ ·æœ¬æ•°

    return input_fn
#  å®šä¹‰æ¨¡å‹ç”Ÿæˆå’Œå‡½æ•°make_model
def make_model(n_classes=10):
    model = tf.keras.Sequential([
    # filters = 32 å·ç§¯æ ¸çš„æ•°é‡ ä¹Ÿå°±æ˜¯ä¸‹ä¸€å±‚çš„è¾“å‡ºä¸ªæ•° kernel_size å·ç§¯æ ¸çš„é«˜åº¦å’Œå®½åº¦
    # ç¬¬ä¸€å±‚æƒé‡å‚æ•°ä¸º filters * kernel_size^2 * D1(è¾“å…¥å±‚çš„æ·±åº¦) = 32*5*5*1(ç°åº¦å›¾)
    # ç¬¬ä¸€å±‚çš„åç½®å‚æ•°ä¸ºfiltersä¸ªåç½® 32
    # parms_1 = 840 + 32 = 832
    # è¾“å…¥å°ºå¯¸ :ã€€28 * 28
    # output_shape = (input_shape - kernel_size + 1) / strides) = (28 - 5) + 1 = 24 * 24
    # (None, 24, 24, 32) è¾“å‡ºæ·±åº¦ = filters = 32
    tf.keras.layers.Conv2D(filters=32 ,kernel_size= (5,5),strides=(1,1),activation=tf.nn.relu,input_shape=(28,28,1)),

    # æ± åŒ–å±‚ä¸å½±å“å‚æ•°ä¸ªæ•° åªæ˜¯è¿›è¡Œæ•°æ®å‹ç¼©é‡‡æ · (None, 12, 12, 32)
    # è¾“å…¥å°ºå¯¸ = 24 * 24
    # poolingåœ¨ä¸åŒçš„depthä¸Šæ˜¯åˆ†å¼€æ‰§è¡Œçš„ æ•… è¾“å‡ºæ·±åº¦ = 32ä¸å˜
    # pool_size=(2, 2)è¡¨ç¤ºæ¯4ä¸ªåƒç´ å¾—åˆ°ä¸€ä¸ªå€¼,æ¯2è¡Œç¼©æˆ1è¡Œ
    # æ•…è¾“å‡ºå°ºå¯¸ä¸º 12 * 12
    tf.keras.layers.MaxPooling2D(pool_size=(2, 2),strides=(2,2), padding='valid'),

    # ç»§ç»­å·ç§¯
    # ç¬¬äºŒå±‚æƒé‡å‚æ•° = 64*3*3*32 = 18432  + 64ä¸ªåç½®å‚æ•° = 18496
    # è¾“å…¥å°ºå¯¸  = 12 * 12
    # è¾“å‡ºå°ºå¯¸ = 12 - 3 + 1 = 10
    # è¾“å‡ºæ·±åº¦ = 64
    # (None, 10, 10, 64)
    tf.keras.layers.Conv2D(filters=64,kernel_size=(3,3),activation=tf.nn.relu),

    # è¾“å…¥å°ºå¯¸ = 10 * 10
    # è¾“å‡ºæ·±åº¦ = 64 ä¸å˜
    # è¾“å‡ºå°ºå¯¸ = 10 / 2 = 5 * 5
    # (None, 5, 5, 64)
    tf.keras.layers.MaxPooling2D(pool_size=(2, 2),strides=(2,2), padding='valid'),

    # å±•å¹³å±‚å‘é‡ç”¨äºè¡”æ¥å…¨è¿æ¥å±‚Dense
    # 5*5*64 = 1600
    tf.keras.layers.Flatten(),

    # ç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚æŒ‡å®š1024ä¸ªç¥ç»å…ƒ 1600*1024 ä¸ªæƒé‡å‚æ•° + 1024 ä¸ªåç½®å‚æ•° = 1639424
    tf.keras.layers.Dense(units=1024,activation=tf.nn.relu),

    # å¼•å…¥Dropouté˜²æ­¢è¿‡æ‹Ÿåˆ
    # è®­ç»ƒæœŸé—´çš„æ¯ä¸€æ­¥ä»¥ rate çš„é¢‘ç‡éšæœºå°†è¾“å…¥å•å…ƒè®¾ç½®ä¸º 0ï¼Œè¿™æœ‰åŠ©äºé˜²æ­¢è¿‡æ‹Ÿåˆã€‚
    # æœªè®¾ç½®ä¸º 0 çš„è¾“å…¥æŒ‰ 1ï¼ˆ1 - rateï¼‰æ”¾å¤§ï¼Œä»¥ä¾¿æ‰€æœ‰è¾“å…¥çš„æ€»å’Œä¿æŒä¸å˜
    tf.keras.layers.Dropout(rate=0.5),

    # æœ€åä¸€å±‚,åˆ†ç±»å±‚çš„å…¨è¿æ¥å±‚ å‚æ•° = 1024*10+10=10250
    tf.keras.layers.Dense(n_classes)
])
    return model
#  å®šä¹‰ä¼°è®¡å™¨ç”Ÿæˆå‡½æ•°model_fn
def model_fn(features,labels,mode):
    # è®­ç»ƒå’Œè®°å½•summaryæ“ä½œå¿…é¡»ä½¿ç”¨tf.compat.v1å…¼å®¹æ¨¡å—
    # ç¡®ä¿é«˜ç‰ˆæœ¬çš„TFæ”¯æŒä½ç‰ˆæœ¬çš„TFä»£ç 
    v1 = tf.compat.v1
    model = make_model(n_classes=10)

    preictions_onehot = model(features) # å¾—åˆ°ç‹¬çƒ­ç¼–ç 

    mode_list = [tf.estimator.ModeKeys.TRAIN ,
                 tf.estimator.ModeKeys.EVAL ,
                 tf.estimator.ModeKeys.PREDICT ]

    # For mode==ModeKeys.TRAIN: éœ€è¦çš„å‚æ•°æ˜¯ loss and train_op.
    # For mode==ModeKeys.EVAL:  éœ€è¦çš„å‚æ•°æ˜¯  loss.
    # For mode==ModeKeys.PREDICT: éœ€è¦çš„å‚æ•°æ˜¯ predictions.
    # å®šä¹‰äº†ä¸€ä¸ªå…·ä½“çš„ä¼°è®¡å™¨æ¨¡å‹å¯¹è±¡

    # 1) PREDICTé˜¶æ®µ
    if mode == mode_list[-1]:
        predictions = v1.argmax(preictions_onehot,-1) # æ‰¾åˆ°ç‹¬çƒ­ç¼–ç 1çš„ä½ç½® ä¹Ÿå°±æ˜¯é¢„æµ‹çš„æ ‡ç­¾
        return tf.estimator.EstimatorSpec(mode,predictions=predictions)

    # reduce_meanä¸­axis=Noneé»˜è®¤è®¡ç®—æ‰€æœ‰ç»´åº¦è¿”å›ä¸€ä¸ªå€¼ axis=0æŒ‰åˆ—è®¡ç®—å¹³å‡å€¼è¿”å›åˆ—å¼ é‡ axis=1æŒ‰è¡Œ
    loss =v1.reduce_mean(v1.nn.sparse_softmax_cross_entropy_with_logits(
        # è®¡ç®—logitså’Œlabelsä¹‹é—´çš„ç¨€ç– softmax äº¤å‰ç†µ
        # æµ‹é‡ç¦»æ•£åˆ†ç±»ä»»åŠ¡ä¸­çš„æ¦‚ç‡è¯¯å·®ï¼Œå…¶ä¸­ç±»æ˜¯äº’æ–¥
        logits=preictions_onehot,
        # ä»å¼ é‡çš„å½¢çŠ¶ä¸­åˆ é™¤å¤§å°ä¸º 1 çš„ç»´åº¦
        labels=v1.squeeze(labels)),
        axis=None)

    # 2) EVALé˜¶æ®µ
    # è®¡ç®—å‡†ç¡®ç‡
    accuary = v1.metrics.accuracy(
        labels=labels , predictions=v1.argmax(preictions_onehot ,-1),name='accuracy')
    metrics = {"accuracy":accuary}
    if mode  == mode_list[-2] :
        # ä½¿ç”¨å‡†ç¡®ç‡æ¥éªŒè¯
        return tf.estimator.EstimatorSpec(mode,loss=loss,eval_metric_ops= metrics)

    # 3) TRAINé˜¶æ®µ
    # è·å–å…¨å±€æ­¥é•¿å¼ é‡ åœ¨train_opçš„å®šä¹‰ä¸­ä½¿ç”¨
    global_step = v1.train.get_global_step()
    if mode == mode_list[0]:
        opt = v1.train.AdamOptimizer(1e-4)
        # é€šè¿‡æ›´æ–° var_list æ·»åŠ æ“ä½œä»¥æœ€å°åŒ– loss
        train_op = opt.minimize (
            loss=loss,
            var_list=model.trainable_variables,
            global_step=global_step
        )
        return tf.estimator.EstimatorSpec(mode,loss=loss,train_op =train_op)
    # æŠ›å‡ºé”™è¯¯ : æ–¹æ³•æˆ–å‡½æ•°å°šæœªå®ç°
    raise NotImplementedError (f"Unknown mode {mode}")
#  å®ä¾‹åŒ–ä¼°è®¡å™¨
#%% å®šä¹‰ä¸¤ç”¨å¯¹è±¡tf.estimator.EstimatorSpec ä½¿ç”¨modeå†³å®šæ¨¡å¼
# tf.estimator.Estimatorå†…éƒ¨å¯¹æ¨¡å‹ç¼–è¯‘è¿‡
estimator = tf.estimator.Estimator(model_fn=model_fn,model_dir="log")
epochs = 5
for epoch in range(epochs) :
    print(f"ç¬¬{epoch}æ¬¡è¿­ä»£")
    # è®­ç»ƒå‡½æ•°éœ€è¦æ•°æ®é›† åªæ˜¯è¿™é‡Œä½¿ç”¨çš„è¾“å…¥æµå‡½æ•°ä»¥è¿­ä»£æ–¹å¼æä¾›æ•°æ®é›†
    estimator.train(get_input_fn(
        mode=tf.estimator.ModeKeys.TRAIN,
        num_epochs= 1
    ))
    estimator.evaluate(get_input_fn(
        mode = tf.estimator.ModeKeys.EVAL
    ))
#%%
# ä¹Ÿå¯ä»¥ä¸å®šä¹‰  tf.estimator.EstimatorSpecå¯¹è±¡
# ç›´æ¥å®šä¹‰train_specå’Œeval_specå¯¹è±¡
train_spec = tf.estimator.TrainSpec(input_fn=get_input_fn(tf.estimator.ModeKeys.TRAIN,num_epochs=5))
eval_spec = tf.estimator.EvalSpec(input_fn=get_input_fn(tf.estimator.ModeKeys.EVAL,num_epochs=1))
model = make_model(10)
# æ¨¡å‹æ‰‹åŠ¨ç¼–è¯‘
model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
# æ¨¡å‹æ‰‹åŠ¨è½¬æ¢ä¸ºä¼°è®¡å™¨
estimator = tf.keras.estimator.model_to_estimator(keras_model=model)
# ä½¿ç”¨train_and_evaluateæ–¹æ³•
tf.estimator.train_and_evaluate(estimator,train_spec,eval_spec)




