#%%
# -*- coding UTF-8 -*-
'''
@Project : MyPythonProjects
@File : 5.æœˆç‰™æ•°æ®é›†(éªŒè¯è¿‡æ‹ŸåˆæŠ€æœ¯).py
@Author : chenbei
@Date : 2021/8/12 11:22
'''
import matplotlib.pyplot as plt
from matplotlib.pylab import mpl

plt.rcParams['font.sans-serif'] = ['Times New Roman']  # è®¾ç½®å­—ä½“é£æ ¼,å¿…é¡»åœ¨å‰ç„¶åè®¾ç½®æ˜¾ç¤ºä¸­æ–‡
mpl.rcParams['font.size'] = 10.5  # å›¾ç‰‡å­—ä½“å¤§å°
mpl.rcParams['font.sans-serif'] = ['SimHei']  # æ˜¾ç¤ºä¸­æ–‡çš„å‘½ä»¤
mpl.rcParams['axes.unicode_minus'] = False  # æ˜¾ç¤ºè´Ÿå·çš„å‘½ä»¤
mpl.rcParams['agg.path.chunksize'] = 10000
plt.rcParams['figure.figsize'] = (7.8, 3.8)  # è®¾ç½®figure_sizeå°ºå¯¸
plt.rcParams['savefig.dpi'] = 600  # å›¾ç‰‡åƒç´ 
plt.rcParams['figure.dpi'] = 600  # åˆ†è¾¨ç‡
from matplotlib.font_manager import FontProperties
font_set = FontProperties(fname=r"C:\Windows\Fonts\simsun.ttc", size=10.5)
import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.datasets import make_moons
from sklearn.model_selection import train_test_split
N_SAMPLES = 1000
TEST_SIZE = 0.2
X, y = make_moons(n_samples = N_SAMPLES, noise=0.25, random_state=100)
X_train, X_test, y_train, y_test = train_test_split(X, y,test_size = TEST_SIZE, random_state=42)
xx = np.linspace(int(X[:,0].min())-1, int(X[:,0].max())+1, N_SAMPLES )
yy = np.linspace(int(X[:,1].min())-1, int(X[:,1].max())+1, N_SAMPLES )
XX , YY = np.meshgrid(xx, yy)
def make_plot(X, y, plot_name, XX=None, YY=None, preds=None):
     X = np.array(X)
     y = np.array(y)
     plt.figure()
     axes = plt.gca()
     axes.set_xlim([X.min()-1,X.max()+1])
     axes.set_ylim([y.min()-1,y.max()+1])
     axes.set(xlabel="$x_1$", ylabel="$x_2$")
     # æ ¹æ®ç½‘ç»œè¾“å‡ºç»˜åˆ¶é¢„æµ‹æ›²é¢
     if(XX is not None and YY is not None and preds is not None):
         # å…ˆå°†1000000*1çš„predsä¹Ÿå°±æ˜¯é¢„æµ‹æ ‡ç­¾é‡å¡‘å½¢çŠ¶ä¸ºäºŒç»´å¹³é¢(1000,1000)åæ ‡çš„å–å€¼ï¼Œè¿™é‡Œæ˜¯é0å³1çš„
         # ç„¶åå¯¹è¿™äº›ç‚¹è¿›è¡Œç»˜åˆ¶
         plt.contourf(XX, YY, preds.reshape(XX.shape), 25, alpha = 0.5,cmap=plt.cm.cool)# ä¸‰ç»´ç­‰é«˜çº¿çš„è½®å»“å¡«å……å›¾
         plt.contour(XX, YY, preds.reshape(XX.shape), levels=[0.5,1],cmap="winter",vmin=0, vmax=.6) # ä¸‰ç»´ç­‰é«˜çº¿çš„è½®å»“çº¿
     # ç»˜åˆ¶æ­£è´Ÿæ ·æœ¬
     postive_x = X[y==1]
     negative_x = X[y==0]
     plt.scatter(postive_x[:, 0], postive_x[:, 1], c='r', s=20, edgecolors='none', marker='o')
     plt.scatter(negative_x[:, 0], negative_x[:, 1], c='b', s=20,  edgecolors='none', marker='+')
     plt.title(plot_name, fontsize=30)
     plt.show()
make_plot(X, y, 'æœˆç‰™æ•°æ®é›†å¯è§†åŒ–')
N_EPOCHS = 100
#%% ã€1ã€‘ç½‘ç»œå±‚æ•°å¯¹æ•°æ®é›†åŒºåˆ†çš„å½±å“
for n in range(5): # æ„å»º 5 ç§ä¸åŒå±‚æ•°çš„ç½‘ç»œ
    model = tf.keras.Sequential()# åˆ›å»ºå®¹å™¨
    # åˆ›å»ºç¬¬ä¸€å±‚ è¾“å‡ºèŠ‚ç‚¹8 è¦æ±‚è¾“å…¥å½¢çŠ¶2
    model.add(tf.keras.layers.Dense(8,input_dim=2,activation='relu'))
    for _ in range(n): # æ·»åŠ  n å±‚ï¼Œå…± n+2 å±‚
        model.add(tf.keras.layers.Dense(32, activation='relu'))
    model.add(tf.keras.layers.Dense(1, activation='sigmoid')) # åˆ›å»ºæœ€æœ«å±‚
    model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy']) # æ¨¡å‹è£…é…ä¸è®­ç»ƒ
    history = model.fit(X_train, y_train, epochs=N_EPOCHS, verbose=1)
    # ç»˜åˆ¶ä¸åŒå±‚æ•°çš„ç½‘ç»œå†³ç­–è¾¹ç•Œæ›²çº¿
    preds = model.predict_classes(np.c_[XX.ravel(), YY.ravel()])
    # æŒ‰è¡Œè¿æ¥ä¸¤ä¸ªå±•å¼€çš„ç½‘æ ¼ç‚¹ 1000000*2 é€å…¥predict_classes
    # ç›¸å½“äºä¸ä»…é¢„æµ‹å®é™…çš„æ•°æ®ï¼Œè€Œæ˜¯å¯¹å®é™…æ•°æ®æ‰€äº§ç”Ÿçš„ç­‰åˆ†1000ä»½çš„æ‰€æœ‰äºŒç»´æ•°æ®ç‚¹éƒ½è¿›è¡Œé¢„æµ‹
    # ä¸€ä¸ªåæ ‡ç‚¹å¾—åˆ°1ä¸ªæ ‡ç­¾preds æ•…preds.shape=(1000000,1)
    title = "éšå«å±‚æ•°({})ç½‘ç»œå®¹é‡({})".format(n,2+n*1)
    make_plot(X_train, y_train, title, XX, YY, preds)
#%% ã€2ã€‘Droputçš„å½±å“
for n in range(5): # æ„å»º 5 ç§ä¸åŒæ•°é‡ Dropout å±‚çš„ç½‘ç»œ
    model = tf.keras.Sequential()  # åˆ›å»º
    # åˆ›å»ºç¬¬ä¸€å±‚ è¾“å‡ºèŠ‚ç‚¹8 è¦æ±‚è¾“å…¥å½¢çŠ¶2
    model.add(tf.keras.layers.Dense(8, input_dim=2, activation='relu'))
    counter = 0
    for _ in range(5):  # ç½‘ç»œå±‚æ•°å›ºå®šä¸º 5
        # n=0 : Dense + 5ä¸ªDense
        # n=1 : Dense + Dense + Dropout + 4ä¸ªDense
        # n=2 ï¼šDense + Dense + Dropout + Dense + Dropout + 3ä¸ªDense
        # ...
        # n=4 ï¼šDense + (Dense,Dropout) + (Dense,Dropout) + (Dense,Dropout) + (Dense,Dropout) + Dense
        model.add(tf.keras.layers.Dense(64, activation='relu'))
        if counter < n:  # æ·»åŠ  n ä¸ª Dropout å±‚ næœ€å¤§å–åˆ°4
            counter += 1
            model.add(tf.keras.layers.Dropout(rate=0.5))
    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # è¾“å‡ºå±‚
    model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy']) # æ¨¡å‹è£…é…
    history = model.fit(X_train, y_train, epochs=N_EPOCHS, verbose=1)
    # ç»˜åˆ¶ä¸åŒ Dropout å±‚æ•°çš„å†³ç­–è¾¹ç•Œæ›²çº¿
    preds = model.predict_classes(np.c_[XX.ravel(),YY.ravel()])
    title = "Dropoutå±‚æ•°({})".format(counter)
    make_plot(X_train, y_train, title, XX, YY, preds)
#%%ã€3ã€‘æ­£åˆ™åŒ–ç³»æ•°çš„å½±å“
def build_model_with_regularization(_lambda):
    # åˆ›å»ºå¸¦æ­£åˆ™åŒ–é¡¹çš„ç¥ç»ç½‘ç»œ
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Dense(8, input_dim=2,activation='relu')) # ä¸å¸¦æ­£åˆ™åŒ–é¡¹
    model.add(tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(_lambda)))# å¸¦ L2 æ­£åˆ™åŒ–é¡¹
    model.add(tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(_lambda)))# å¸¦ L2 æ­£åˆ™åŒ–é¡¹
    model.add(tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(_lambda)))# å¸¦ L2 æ­£åˆ™åŒ–é¡¹
    # è¾“å‡ºå±‚
    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))
    model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy']) # æ¨¡å‹è£…é…
    return model
def plot_weights_matrix(model,weightIndex,plot_title) :
    # weightIndex æŒ‡å®šç»˜åˆ¶çš„æƒé‡
    trainable_weights  = model.trainable_weights
    weights = trainable_weights[weightIndex].numpy()
    # np.set_printoptions(precision=4,suppress=True)
    # print("W2æƒå€¼ï¼šå¹³å‡å€¼{} æœ€å¤§å€¼{} æœ€å°å€¼{}".format(np.mean(weights),weights.max(),weights.min()))
    xx = np.linspace(int(weights.min())-1, int(weights.max())+1, len(weights))
    yy = np.linspace(int(weights.min())-1, int(weights.max())+1, len(weights))
    XX , YY = np.meshgrid(xx,yy)
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')
    ax.plot_surface(XX,YY,weights,cmap = plt.get_cmap('rainbow'))
    ax.set_zlim(-int(weights.min())-0.5, int(weights.max())+0.5)
    ax.set_xlabel("w2")
    ax.set_ylabel("w2")
    para = [np.round(weights.max(),2),np.round(weights.min(),2),np.round(np.mean(weights),2)]
    ax.set_title(plot_title+" W{:d}ï¼šæœ€å¤§å€¼{:.5f} æœ€å°å€¼{:.5f} å¹³å‡å€¼{:.5f}"
                 .format(weightIndex//2,para[0],para[1],para[-1]))
    plt.show()
# ä¿æŒç½‘ç»œç»“æ„ä¸å˜çš„æ¡ä»¶ä¸‹ï¼Œè°ƒèŠ‚æ­£åˆ™åŒ–ç³»æ•°ğœ†æµ‹è¯•ç½‘ç»œçš„è®­ç»ƒæ•ˆæœ
def model_test_lambda(_lambdas,weightIndex):
    for _lambda in _lambdas :
        model = build_model_with_regularization(_lambda)
        history = model.fit(X_train, y_train, epochs=N_EPOCHS, verbose=1)
        # ç»˜åˆ¶æƒå€¼èŒƒå›´
        plot_title = "lambda = {}".format(str(_lambda))
        # ç»˜åˆ¶ç½‘ç»œæƒå€¼èŒƒå›´å›¾
        plot_weights_matrix(model,weightIndex=weightIndex,plot_title=plot_title)
        preds = model.predict_classes(np.c_[XX.ravel(), YY.ravel()])
        title = "æ­£åˆ™åŒ–{}".format(_lambda)
        #make_plot(X_train, y_train, title, XX, YY, preds)
_lambdas = [0.00001,0.001,0.1,0.13]
model_test_lambda(_lambdas=_lambdas,weightIndex=4)
#%%




